{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtJeqdFyV00t"
   },
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field of computer science that focuses on the interaction between human language and computers. NLP enables computers to understand, interpret, and generate human language. NLP has many applications in journalism from performing complex analysis over written documents to generating parts of articles. In this notebook, we will cover the core concepts of NLP before moving on to some advanced applications.\n",
    "\n",
    "* **Setup and Packages**: There are several key NLP packages that provide a lot of functionality and are important to know. We will provide a basic overview of each, and will then use them in the following sections.\n",
    "* **Tokenization**: This critical step involves taking the original text and breaking it up into pieces such as words which is critical for many algorithms.\n",
    "* **Part-of-Speech (POS) Tagging**: Grammar is a core part of every language and identifying the verbs, subjects, and objects of a sentence is a foundational task for understanding language.\n",
    "* **Dependency Parsing**: Dependency parsing is similar to POS Tagging, but instead of just finding what part of speech each word is, it finds how the words are connected. For example, we can use this to find what part of the sentence the verb acts on. For example, in the sentence \"The boy kicks the ball,\" the direct object of \"kicks\" is \"ball\". \n",
    "* **Named Entity Recognition (NER)**: Knowing who was mentioned in a text can be a very useful analysis, and we will cover how to extract names of people and businesses.\n",
    "\n",
    "In the next section, we will cover representing words as \"vectors\", sentiment analysis, meaning similarity, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG6PVHwsyu60"
   },
   "source": [
    "## Setup and Packages\n",
    "\n",
    "There are three key NLP packages we will learn and use: NLTK, Hazm, spaCy, and Gensim.\n",
    "\n",
    "### NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is one of the most widely used Python NLP packages. It provides many utilities for all the common NLP tasks. We will use this extensively in this notebook.\n",
    "\n",
    "We will import it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tc3SChEY1cmr"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJV_hqwV2YWL"
   },
   "source": [
    "## Hazm\n",
    "\n",
    "For Persian NLP analysis, Hazam is a library that provides many core NLP algorithms for Persian text and is compatible with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JCk5eRv42kFv"
   },
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLQ1FEM01dIk"
   },
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy is a modern and efficient library that is used in many production applications. This can be used both for small analysis and for building a complex NLP system that handles thousands of documents quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PekaIrn1zuM",
    "outputId": "37856531-7679-4cc4-ed69-b410db74e4c5"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oas-43HyuQ_"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "One foundation of NLP is tokenization. Tokenization is the process of splitting text into individual words, phrases, or symbols, known as tokens. Tokenization is the first step in any NLP pipeline, as it enables us to analyze text at a more granular level. Just as humans can read or analyze speech by understanding each word invididually, computers can find and then analyze using these individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LO_eqll4xB5",
    "outputId": "040aae90-2f35-4685-e20d-14b375c584ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['این', 'یک', 'جمله', 'نمونه', 'است', '.']\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"این یک جمله نمونه است.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na9zIqzU6EvN"
   },
   "source": [
    "As we can see, the sentence is now split into each individual word. We can now perform an analysis such as getting the total word count, or the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bFfHXwqa03HE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Show word count\n",
    "print(len(tokens))\n",
    "\n",
    "long_sentence = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5836RuA0-Zn"
   },
   "source": [
    "#### Sentence Tokenization\n",
    "\n",
    "Some tasks will also want us to analyze a document sentence by sentence, and to do this, we want to split the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9f_h_0h1M9N",
    "outputId": "058ab2ce-4722-4ffc-8652-ff763be307dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام دنیا!', 'این یک متن فارسی است.', 'امیدوارم که این درس مفید باشد.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentence tokenizer\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"سلام دنیا! این یک متن فارسی است. امیدوارم که این درس مفید باشد.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Print the sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHCPsN5Q1UT9"
   },
   "source": [
    "Now, as an exercise, see how we can do word tokenization for each of the sentences. The first way uses a `for` loop and the second uses a more advanced concept called list comprehension, but this is not necessary to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JbridNc1hav",
    "outputId": "6cfffdbb-7358-44be-b324-b252d4746004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [['گربه', 'در', 'خانه', 'است', '.'], ['او', 'به', 'دنبال', 'موش', 'است', '.'], ['موش', 'در', 'حال', 'حاضر', 'در', 'حیاط', 'پنهان', 'شده_است', '.'], ['گربه', 'دوباره', 'به', 'دنبال', 'موش', 'است', '.'], ['موش', 'سعی', 'می', 'کند', 'از', 'گربه', 'فرار', 'کند', '.'], ['گربه', 'در', 'خانه', 'است', '.'], ['گربه', 'بازی', 'می', 'کند', '.'], ['موش', 'بازی', 'می', 'کند', '.'], ['گربه', 'در', 'خانه', 'است', '.'], ['گربه', 'به', 'دنبال', 'موش', 'است', '.']]\n",
      "Tokenized: [['گربه', 'در', 'خانه', 'است', '.'], ['او', 'به', 'دنبال', 'موش', 'است', '.'], ['موش', 'در', 'حال', 'حاضر', 'در', 'حیاط', 'پنهان', 'شده_است', '.'], ['گربه', 'دوباره', 'به', 'دنبال', 'موش', 'است', '.'], ['موش', 'سعی', 'می', 'کند', 'از', 'گربه', 'فرار', 'کند', '.'], ['گربه', 'در', 'خانه', 'است', '.'], ['گربه', 'بازی', 'می', 'کند', '.'], ['موش', 'بازی', 'می', 'کند', '.'], ['گربه', 'در', 'خانه', 'است', '.'], ['گربه', 'به', 'دنبال', 'موش', 'است', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in each sentence\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "  tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "print('Tokenized:', tokenized_sentences)\n",
    "\n",
    "# Tokenizing each sentence using list comprehension\n",
    "print('Tokenized:', [tokenizer.tokenize(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz_135jk2QAq"
   },
   "source": [
    "#### Example Usage\n",
    "\n",
    "Now we will take a several sentence example and return the longest sentence, and then the three most common word tokens. We use the `Counter` item from the `collections` library which will help us get the most common word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3uFNZCt2x-U",
    "outputId": "84563924-8477-4aa2-c597-e08a6b45e5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. '.' appears 10 times.\n",
      "2. 'گربه' appears 7 times.\n",
      "3. 'است' appears 6 times.\n",
      "Longest sentence: 'موش در حال حاضر در حیاط پنهان شده است.'\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from hazm import WordTokenizer, SentenceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"\"\"\n",
    "گربه در خانه است. او به دنبال موش است. موش در حال حاضر در حیاط پنهان شده است.\n",
    "گربه دوباره به دنبال موش است. موش سعی می کند از گربه فرار کند. گربه در خانه است.\n",
    "گربه بازی می کند. موش بازی می کند. گربه در خانه است. گربه به دنبال موش است.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the tokenizers\n",
    "word_tokenizer = WordTokenizer()\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Tokenize the text into words and sentences\n",
    "words = word_tokenizer.tokenize(text)\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counter = Counter(words)\n",
    "\n",
    "# Find the top three most common words\n",
    "top_three_common_words = word_counter.most_common(3)\n",
    "\n",
    "# Find the longest sentence\n",
    "longest_sentence = max(sentences, key=len)\n",
    "\n",
    "# Print the results\n",
    "for i, (word, count) in enumerate(top_three_common_words, start=1):\n",
    "    print(f\"{i}. '{word}' appears {count} times.\")\n",
    "print(f\"Longest sentence: '{longest_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1K_EbHg4FH1"
   },
   "source": [
    "Notice here that `.` is counted as it's own token, so we would need to remove it from the tokens if we don't want it included or counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9soKP3Zm4PXi"
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "POS tagging is the process of assigning a grammatical category or part-of-speech to each word in a sentence. This is useful so we can remove things like the word \"and\" and just get a collection of nouns or other items.\n",
    "\n",
    "To do this, the `hazm` library has trained a tagger model which can be downloaded from their [GitHub page](https://github.com/roshan-research/hazm), but it has already been downloaded and placed in the `resources/` folder of this repository. We are two directories away from the repository root, so we need to put `../../` first in the path to reach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "alS0vQgI4pJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('گربه', 'N'), ('در', 'P'), ('حال', 'Ne'), ('بازی', 'N'), ('کردن', 'N'), ('با', 'P'), ('موش', 'N'), ('است', 'V')]\n"
     ]
    }
   ],
   "source": [
    "from hazm import POSTagger\n",
    "\n",
    "# Initialize the tagger\n",
    "tagger = POSTagger(model='../../resources/postagger.model')\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"گربه در حال بازی کردن با موش است\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokenizer = WordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = tagger.tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb-tRmCJ40_8"
   },
   "source": [
    "Notice here that each word now has a part of speech associated with it. We can see that `N` shows the nows and then `V` shows the verb.\n",
    "\n",
    "To find all the nouns, all we have to do is either of the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['گربه', 'بازی', 'کردن', 'موش']\n",
      "Verbs: ['است']\n",
      "Nouns: ['گربه', 'بازی', 'کردن', 'موش']\n"
     ]
    }
   ],
   "source": [
    "all_nouns = []\n",
    "all_verbs = []\n",
    "for pos_tag in pos_tags:\n",
    "    if pos_tag[1] == 'N':\n",
    "        all_nouns.append(pos_tag[0])\n",
    "    elif pos_tag[1] == 'V':\n",
    "        all_verbs.append(pos_tag[0])\n",
    "print('Nouns:', all_nouns)\n",
    "print('Verbs:', all_verbs)\n",
    "\n",
    "# or a more advanced but equal way\n",
    "print('Nouns:', [pos_tag[0] for pos_tag in pos_tags if pos_tag[1] == 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is useful for finding individual words, but another useful approach is to find the different phrases in a sentence, such as the noun phrases and verb phrases. A \"chunker\" is a model that can find the groups of words that together make up the individual phrases in a sentence. `hazm` provides a chunker model as well, which we will now use.\n",
    "\n",
    "Once again, we need to load the chunker model which has already been downloaded and placed in the `resources/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[گربه NP] [در PP] [حال بازی کردن NP] [با PP] [موش NP] [است VP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = Chunker(model='../../resources/chunker.model')\n",
    "tagged = tagger.tag(word_tokenize('گربه در حال بازی کردن با موش است'))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"VP\" stands for \"verb phrase\", \"NP\" stands for \"noun phrase\" and \"PP\" stands for \"prepositional phrase\".\n",
    "\n",
    "We will now chunk a simple sentence to show how it breaks into just three phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[سگ زرد NP] [را POSTP] [دیدیم VP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentence = \"سگ زرد را دیدیم\"\n",
    "tagged = tagger.tag(word_tokenize(simple_sentence))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tree2brackets` provides an easy way to view the sentence broken into phrases, but we can also view the original data that the chunker extracts. It has what is called a \"tree\" structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP سگ/Ne زرد/AJ) (POSTP را/POSTP) (VP دیدیم/V))\n"
     ]
    }
   ],
   "source": [
    "chunked = chunker.parse(tagged)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP سگ/Ne زرد/AJ)\n",
      "('سگ', 'Ne')\n"
     ]
    }
   ],
   "source": [
    "# Get the first phrase\n",
    "print(chunked[0])\n",
    "# Get the first word of the first phrase\n",
    "print(chunked[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to get the verb phrase, we can do a `for` loop until we find the phrase with type `'VP'`. This is done by looking at the `.label()` value of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP دیدیم/V)\n"
     ]
    }
   ],
   "source": [
    "verb_phrase = None\n",
    "for chunk in chunked:\n",
    "    if chunk.label() == 'VP':\n",
    "        verb_phrase = chunk\n",
    "print(verb_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the structure of language is key for successful NLP, and we have now learned several ways that we can automatically find the part-of-speech and the phrase structure of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "We have now learned some core NLP analysis approaches, but these focused on finding either the part of speech of individual words or finding a few groupings of words. We now want to find how each word connects to each other. This is called dependency parsing and lets us find, for example, which item in a sentence is the direct object of the verb, which noun is the subject among all the other dependencies that make up a sentence.\n",
    "\n",
    "Before we can do dependency parsing though, there is one more concept to quickly learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of taking different forms of the same word and grouping them together. For example, \"sell\" and \"sells\" are both in the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سگ\n",
      "سگ\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize('سگ'))\n",
    "print(lemmatizer.lemmatize('سگها'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "\n",
    "We are now ready to perform dependency parsing. We will take the simple sentence from before and parse out its dependencies.\n",
    "\n",
    "Note: `java` needs to be installed for the dependency parser to work. If you receive an error when trying to run, that is fine, and just try to understand what the code is doing. If you need to do dependency parsing, then simply install [Open JDK](https://openjdk.org/) and the specific instructions will depend on what operating system you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7f75ac37a5e0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [4]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'Ne',\n",
      "                 'deps': defaultdict(<class 'list'>, {'NPOSTMOD': [2]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': 'سگ',\n",
      "                 'rel': 'PREDEP',\n",
      "                 'tag': 'Ne',\n",
      "                 'word': 'سگ'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'AJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 1,\n",
      "                 'lemma': 'زرد',\n",
      "                 'rel': 'NPOSTMOD',\n",
      "                 'tag': 'AJ',\n",
      "                 'word': 'زرد'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'POSTP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'PREDEP': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'را',\n",
      "                 'rel': 'OBJ',\n",
      "                 'tag': 'POSTP',\n",
      "                 'word': 'را'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'V',\n",
      "                 'deps': defaultdict(<class 'list'>, {'OBJ': [3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'دید#بین',\n",
      "                 'rel': 'ROOT',\n",
      "                 'tag': 'V',\n",
      "                 'word': 'دیدیم'}})\n"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'سگ زرد را دیدیم'\n",
    "parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer, working_dir='../../resources/')\n",
    "print(parser.parse(word_tokenize(simple_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original output is hard to read, so we will display it in an easier to interpret format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK\n",
    "\n",
    "Now that we have used `hazm` to do these NLP analysis on Persian text, we will learn how to do the exact same operations using `nltk` in case we want to do an analysis on a text written in a different language. The names are basically the same, so we will quickly go through each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to download some models so `nltk` can do part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/noah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run part-of-speech tagging, and then show chunking all on an English example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'VBG'), ('with', 'IN'), ('NLTK', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence in English\n",
    "sentence = \"I am learning Natural Language Processing with NLTK\"\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(word_tokenize(sentence))\n",
    "print('POS tags:', pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: (S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  learning/VBG\n",
      "  Natural/NNP\n",
      "  Language/NNP\n",
      "  Processing/VBG\n",
      "  with/IN\n",
      "  NLTK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "print('Chunks:', tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the part of speech tags are different than the ones we saw in `hazm` because the model we downloaded used the Penn Part of Speech Tags convention. Here \"I\" is a `PRP` which stands for \"personal pronoun\". The list of all part of speech types in the Penn Part of Speech Tags is available at [this NYU webpage](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html).\n",
    "\n",
    "To get all nouns, one needs to find all the parts of speech that are noun types and then see if that words part of speech matches any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['I', 'Natural', 'Language', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "# create a list of all the part of speech tags we want to match\n",
    "all_noun_types = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n",
    "\n",
    "# now go through the tags and add any that are in our list\n",
    "nouns = []\n",
    "for tag in pos_tags:\n",
    "    if tag[1] in all_noun_types:\n",
    "        nouns.append(tag[0])\n",
    "\n",
    "print('Nouns:', nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "The final part of this introduction will now show how to do Named Entity Recognition (NER) using the spaCy library. NER finds proper nouns in a sentence so that we can know which people, businesses and other entities are mentioned.\n",
    "\n",
    "NER is a statistical approach meaning that some training or data approach is used so it may not find all entities depending upon the quality of the model used. spaCy's Persian option does well usually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
