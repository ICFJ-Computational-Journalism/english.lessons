{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtJeqdFyV00t"
   },
   "source": [
    "# 5.1 Introduction to NLP\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field of computer science that focuses on the interaction between human language and computers. NLP enables computers to understand, interpret, and generate human language. NLP has many applications in journalism from performing complex analysis over written documents to generating parts of articles. In this notebook, we will cover the core concepts of NLP and try an advanced method called Named Entity Recognition.\n",
    "\n",
    "* **Setup and Packages**: There are several key NLP packages that provide a lot of functionality and are important to know. We will provide a basic overview of each, and will then use them in the following sections.\n",
    "* **Tokenization**: This critical step involves taking the original text and breaking it up into pieces such as words which is critical for many algorithms.\n",
    "* **Part-of-Speech (POS) Tagging**: Grammar is a core part of every language and identifying the verbs, subjects, and objects of a sentence is a foundational task for understanding language.\n",
    "* **Dependency Parsing**: Dependency parsing is similar to POS Tagging, but instead of just finding what part of speech each word is, it finds how the words are connected. For example, we can use this to find what part of the sentence the verb acts on. For example, in the sentence \"The boy kicks the ball,\" the direct object of \"kicks\" is \"ball\". \n",
    "* **Named Entity Recognition (NER)**: Knowing who was mentioned in a text can be a very useful analysis, and we will cover how to extract names of people and businesses.\n",
    "\n",
    "In the next notebook, we will cover representing words as \"vectors\", sentiment analysis, meaning similarity, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG6PVHwsyu60"
   },
   "source": [
    "## Setup and Packages\n",
    "\n",
    "There are three key NLP packages we will learn and use in this notebook: NLTK, Hazm, and spaCy\n",
    "\n",
    "### NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is one of the most widely used Python NLP packages. It provides many utilities for all the common NLP tasks. It does not offer specific Persian support, but many tools, use it and it is very good for English tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tc3SChEY1cmr"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJV_hqwV2YWL"
   },
   "source": [
    "### Hazm\n",
    "\n",
    "For Persian NLP analysis, Hazam is a library that provides many core NLP algorithms for Persian text and is compatible with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JCk5eRv42kFv"
   },
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLQ1FEM01dIk"
   },
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy (pronounced \"spacy\") is a modern and efficient library that is used in many production applications. This can be used both for small analysis and for building a complex NLP system that handles thousands of documents quickly. It has basic support for Persian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PekaIrn1zuM",
    "outputId": "37856531-7679-4cc4-ed69-b410db74e4c5"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oas-43HyuQ_"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "One foundation of NLP is tokenization. Tokenization is the process of splitting text into individual words, phrases, or symbols, known as tokens. Tokenization is the first step in any NLP pipeline, as it enables us to analyze text at a more granular level. Just as humans can read or analyze speech by understanding each word invididually, computers can find and then analyze using these individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LO_eqll4xB5",
    "outputId": "040aae90-2f35-4685-e20d-14b375c584ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['این', 'یک', 'جمله', 'نمونه', 'است', '.']\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"این یک جمله نمونه است.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na9zIqzU6EvN"
   },
   "source": [
    "As we can see, the sentence is now split into each individual word. We can now perform an analysis such as getting the total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bFfHXwqa03HE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Show word count\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5836RuA0-Zn"
   },
   "source": [
    "#### Sentence Tokenization\n",
    "\n",
    "Some tasks will also want us to analyze a document sentence by sentence, and to do this, we want to split the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9f_h_0h1M9N",
    "outputId": "058ab2ce-4722-4ffc-8652-ff763be307dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام دنیا!', 'این یک متن فارسی است.', 'امیدوارم که این درس مفید باشد.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentence tokenizer\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"سلام دنیا! این یک متن فارسی است. امیدوارم که این درس مفید باشد.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Print the sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHCPsN5Q1UT9"
   },
   "source": [
    "Now, as an exercise, see how we can do word tokenization for each of the sentences. The first way uses a `for` loop and the second uses a more advanced concept called list comprehension, but this is not necessary to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JbridNc1hav",
    "outputId": "6cfffdbb-7358-44be-b324-b252d4746004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [['سلام', 'دنیا', '!'], ['این', 'یک', 'متن', 'فارسی', 'است', '.'], ['امیدوارم', 'که', 'این', 'درس', 'مفید', 'باشد', '.']]\n"
     ]
    }
   ],
   "source": [
    "# First way: Tokenize the words in each sentence\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "  tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "print('Tokenized:', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [['سلام', 'دنیا', '!'], ['این', 'یک', 'متن', 'فارسی', 'است', '.'], ['امیدوارم', 'که', 'این', 'درس', 'مفید', 'باشد', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Second way: Tokenizing each sentence using list comprehension\n",
    "print('Tokenized:', [tokenizer.tokenize(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz_135jk2QAq"
   },
   "source": [
    "#### Example Usage\n",
    "\n",
    "Now we will take a several sentence example and return the longest sentence, and then the three most common word tokens. We use the `Counter` item from the `collections` library which will help us get the most common word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3uFNZCt2x-U",
    "outputId": "84563924-8477-4aa2-c597-e08a6b45e5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. '.' appears 10 times.\n",
      "2. 'گربه' appears 7 times.\n",
      "3. 'است' appears 6 times.\n",
      "Longest sentence: 'موش در حال حاضر در حیاط پنهان شده است.'\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from hazm import WordTokenizer, SentenceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"\"\"\n",
    "گربه در خانه است. او به دنبال موش است. موش در حال حاضر در حیاط پنهان شده است.\n",
    "گربه دوباره به دنبال موش است. موش سعی می کند از گربه فرار کند. گربه در خانه است.\n",
    "گربه بازی می کند. موش بازی می کند. گربه در خانه است. گربه به دنبال موش است.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the tokenizers\n",
    "word_tokenizer = WordTokenizer()\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Tokenize the text into words and sentences\n",
    "words = word_tokenizer.tokenize(text)\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counter = Counter(words)\n",
    "\n",
    "# Find the top three most common words\n",
    "top_three_common_words = word_counter.most_common(3)\n",
    "\n",
    "# Find the longest sentence\n",
    "longest_sentence = max(sentences, key=len)\n",
    "\n",
    "# Print the results\n",
    "for i, (word, count) in enumerate(top_three_common_words, start=1):\n",
    "    print(f\"{i}. '{word}' appears {count} times.\")\n",
    "print(f\"Longest sentence: '{longest_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1K_EbHg4FH1"
   },
   "source": [
    "Notice here that `.` is counted as it's own token, so we would need to remove it from the tokens if we don't want it included or counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9soKP3Zm4PXi"
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "POS tagging is the process of assigning a grammatical category or part-of-speech to each word in a sentence. This is useful so we can remove things like the word \"and\" and just get a collection of nouns or other items.\n",
    "\n",
    "To do this, the `hazm` library has trained a tagger model which can be downloaded from their [GitHub page](https://github.com/roshan-research/hazm), but, for this notebook, it has already been downloaded and placed in the `resources/` folder of this repository. We are two directories away from the repository root, so we need to put `../../` first in the path to reach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "alS0vQgI4pJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('گربه', 'N'), ('در', 'P'), ('حال', 'Ne'), ('بازی', 'N'), ('کردن', 'N'), ('با', 'P'), ('موش', 'N'), ('است', 'V')]\n"
     ]
    }
   ],
   "source": [
    "from hazm import POSTagger\n",
    "\n",
    "# Initialize the tagger\n",
    "tagger = POSTagger(model='../../resources/postagger.model')\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"گربه در حال بازی کردن با موش است\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokenizer = WordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = tagger.tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb-tRmCJ40_8"
   },
   "source": [
    "Notice here that each word now has a part of speech associated with it. We can see that `N` shows the nouns and then `V` shows the verb.\n",
    "\n",
    "To find all the nouns, all we have to do is either of the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['گربه', 'بازی', 'کردن', 'موش']\n",
      "Verbs: ['است']\n"
     ]
    }
   ],
   "source": [
    "all_nouns = []\n",
    "all_verbs = []\n",
    "for pos_tag in pos_tags:\n",
    "    if pos_tag[1] == 'N':\n",
    "        all_nouns.append(pos_tag[0])\n",
    "    elif pos_tag[1] == 'V':\n",
    "        all_verbs.append(pos_tag[0])\n",
    "print('Nouns:', all_nouns)\n",
    "print('Verbs:', all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['گربه', 'بازی', 'کردن', 'موش']\n"
     ]
    }
   ],
   "source": [
    "# or a more advanced but equal way\n",
    "print('Nouns:', [pos_tag[0] for pos_tag in pos_tags if pos_tag[1] == 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "The above is useful for finding individual words, but another useful approach is to find the different phrases in a sentence, such as the noun phrases and verb phrases. A \"chunker\" is a model that can find the groups of words that together make up the individual phrases in a sentence. `hazm` provides a chunker model as well, which we will now use.\n",
    "\n",
    "Once again, we need to load the chunker model which has already been downloaded and placed in the `resources/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[گربه NP] [در PP] [حال بازی کردن NP] [با PP] [موش NP] [است VP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = Chunker(model='../../resources/chunker.model')\n",
    "tagged = tagger.tag(word_tokenize('گربه در حال بازی کردن با موش است'))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"VP\" stands for \"verb phrase\", \"NP\" stands for \"noun phrase\" and \"PP\" stands for \"prepositional phrase\".\n",
    "\n",
    "We will now chunk a simple sentence to show how it breaks into just three phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[سگ زرد NP] [را POSTP] [دیدیم VP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentence = \"سگ زرد را دیدیم\"\n",
    "tagged = tagger.tag(word_tokenize(simple_sentence))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tree2brackets` provides an easy way to view the sentence broken into phrases, but we can also view the original data that the chunker extracts. It has what is called a \"tree\" structure, where each word  is connected to each other word and some of them are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP سگ/Ne زرد/AJ) (POSTP را/POSTP) (VP دیدیم/V))\n"
     ]
    }
   ],
   "source": [
    "chunked = chunker.parse(tagged)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP سگ/Ne زرد/AJ)\n",
      "('سگ', 'Ne')\n"
     ]
    }
   ],
   "source": [
    "# Get the first phrase\n",
    "print(chunked[0])\n",
    "# Get the first word of the first phrase\n",
    "print(chunked[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to get the verb phrase, we can do a `for` loop until we find the phrase with type `'VP'`. This is done by looking at the `.label()` value of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP دیدیم/V)\n"
     ]
    }
   ],
   "source": [
    "verb_phrase = None\n",
    "for chunk in chunked:\n",
    "    if chunk.label() == 'VP':\n",
    "        verb_phrase = chunk\n",
    "print(verb_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the structure of language is key for successful NLP, and we have now learned several ways that we can automatically find the part-of-speech and the phrase structure of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "We have now learned some core NLP analysis approaches, but these focused on finding either the part of speech of individual words or finding a few groupings of words. We now want to find how each word connects to each other. This is called dependency parsing and lets us find, for example, which item in a sentence is the direct object of the verb or which noun is the subject among all the other dependencies that make up a sentence.\n",
    "\n",
    "Before we can do dependency parsing though, there is one more concept to quickly learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of taking different forms of the same word and grouping them together. For example, \"sell\" and \"sells\" are both in the same group, and \"dog\" and \"dogs\" are grouped together as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سگ\n",
      "سگ\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize('سگ'))\n",
    "print(lemmatizer.lemmatize('سگها'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example usage of this technique is we might want to search for a word that could be written multiple ways. We can lemmatize each of the words and then see if it matches the lemmatization of the word we are searching for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "\n",
    "We are now ready to perform dependency parsing. We will take the simple sentence from before and parse out its dependencies.\n",
    "\n",
    "Note: `java` needs to be installed for the dependency parser to work. If you receive an error when trying to run, that is fine, and just try to understand what the code is doing. If you need to do dependency parsing, then simply install [Open JDK](https://openjdk.org/) and the specific instructions will depend on what operating system you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7f89041bb670>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [4]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'Ne',\n",
      "                 'deps': defaultdict(<class 'list'>, {'NPOSTMOD': [2]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': 'سگ',\n",
      "                 'rel': 'PREDEP',\n",
      "                 'tag': 'Ne',\n",
      "                 'word': 'سگ'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'AJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 1,\n",
      "                 'lemma': 'زرد',\n",
      "                 'rel': 'NPOSTMOD',\n",
      "                 'tag': 'AJ',\n",
      "                 'word': 'زرد'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'POSTP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'PREDEP': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'را',\n",
      "                 'rel': 'OBJ',\n",
      "                 'tag': 'POSTP',\n",
      "                 'word': 'را'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'V',\n",
      "                 'deps': defaultdict(<class 'list'>, {'OBJ': [3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'دید#بین',\n",
      "                 'rel': 'ROOT',\n",
      "                 'tag': 'V',\n",
      "                 'word': 'دیدیم'}})\n"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'سگ زرد را دیدیم'\n",
    "parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer, working_dir='../../resources/')\n",
    "dependency_graph = parser.parse(word_tokenize(simple_sentence))\n",
    "print(dependency_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original output of the raw data is hard to interpret, so we will display it in an easier to interpret format.\n",
    "\n",
    "The first one to be aware of is the CoNLL format. Here we can see the `ROOT` line at the bottom shows the start of the dependency graph which begins with the verb. Then, we can see the `OBJ` line which is the object the verb is acting on. This is a good way to see which role each word has, but it is hard to see how each word connects to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سگ\tNe\t3\tPREDEP\n",
      "زرد\tAJ\t1\tNPOSTMOD\n",
      "را\tPOSTP\t4\tOBJ\n",
      "دیدیم\tV\t0\tROOT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dependency_graph.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one is familiar with graph data, another option is to show it using the `to_dot` method which is a way of displaying and visualizing graphs. We can see here that each word has an associated number and then each number points to another one. We can see below that we start with `0` which then points to item `4` which is the `ROOT` (this is shown by `0 -> 4` on the second line). Then, 4 points to 3, where 3 is the object, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph G{\n",
      "edge [dir=forward]\n",
      "node [shape=plaintext]\n",
      "\n",
      "0 [label=\"0 (None)\"]\n",
      "0 -> 4 [label=\"ROOT\"]\n",
      "1 [label=\"1 (سگ)\"]\n",
      "1 -> 2 [label=\"NPOSTMOD\"]\n",
      "2 [label=\"2 (زرد)\"]\n",
      "3 [label=\"3 (را)\"]\n",
      "3 -> 1 [label=\"PREDEP\"]\n",
      "4 [label=\"4 (دیدیم)\"]\n",
      "4 -> 3 [label=\"OBJ\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(dependency_graph.to_dot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also visualize the sentence using parentheses which show which words form the center of the sentence and then how they connect to the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(دیدیم (را (سگ زرد)))\n"
     ]
    }
   ],
   "source": [
    "dependency_graph.tree().pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Dependency Graph\n",
    "\n",
    "Now that we have created and displayed a dependency graph, we will now navigate one to show how we can get the root and objects of a sentence.\n",
    "\n",
    "The key idea is that a dependency graph is made up of \"nodes\" and each node has a root that is its parent node and then has children nodes. A node here is an actual word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 4,\n",
       " 'word': 'دیدیم',\n",
       " 'lemma': 'دید#بین',\n",
       " 'ctag': 'V',\n",
       " 'tag': 'V',\n",
       " 'feats': '_',\n",
       " 'head': 0,\n",
       " 'deps': defaultdict(list, {'OBJ': [3]}),\n",
       " 'rel': 'ROOT'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will first visualize the root node\n",
    "dependency_graph.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here we can get the word from the `'word'` key and the relationship type by the `'rel'` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  دیدیم\n",
      "Relationship:  ROOT\n"
     ]
    }
   ],
   "source": [
    "root_word = dependency_graph.root['word']\n",
    "root_relationship = dependency_graph.root['rel']\n",
    "print('Word: ', root_word)\n",
    "print('Relationship: ', root_relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the `OBJ` of the sentence and any word that directly connects to the object, as often an object can be made up of multiple words. This will do a basic graph search and will perform what is called breadth first search. The key idea is that we want to search every node in the graph, so we start with the \"root\" node and then add the nodes that are below it and continue doing this until we've added and then searched all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 1,\n",
       " 'word': 'سگ',\n",
       " 'lemma': 'سگ',\n",
       " 'ctag': 'Ne',\n",
       " 'tag': 'Ne',\n",
       " 'feats': '_',\n",
       " 'head': 3,\n",
       " 'deps': defaultdict(list, {'NPOSTMOD': [2]}),\n",
       " 'rel': 'PREDEP'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_graph.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects:\n",
      "را\n",
      "سگ\n"
     ]
    }
   ],
   "source": [
    "# we will create a list which we will add all the nodes too, starting with the root\n",
    "nodes_to_search = [dependency_graph.root]\n",
    "# we will store the OBJ of the sentence here\n",
    "object_nodes = []\n",
    "\n",
    "while len(nodes_to_search) > 0:\n",
    "    # get and remove the first item from the nodes_to_search list\n",
    "    current_node = nodes_to_search.pop()\n",
    "    is_node_object = False\n",
    "    if current_node['rel'] == 'OBJ':\n",
    "        object_nodes.append(current_node)\n",
    "        # now indicate to add any directly connected words\n",
    "        is_node_object = True\n",
    "\n",
    "    \n",
    "    # now add this nodes dependencies to the nodes_to_search\n",
    "    for dependency in current_node['deps'].values():\n",
    "        for node_index in dependency:\n",
    "            nodes_to_search.append(dependency_graph.nodes[node_index])\n",
    "            # add the dependent nodes to object_nodes as well if the current node is an object node\n",
    "            if is_node_object:\n",
    "                object_nodes.append(dependency_graph.nodes[node_index])\n",
    "\n",
    "print('Objects:')\n",
    "for obj in object_nodes:\n",
    "    print(obj['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyze a more complex sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph G{\n",
      "edge [dir=forward]\n",
      "node [shape=plaintext]\n",
      "\n",
      "0 [label=\"0 (None)\"]\n",
      "0 -> 6 [label=\"ROOT\"]\n",
      "1 [label=\"1 (.)\"]\n",
      "2 [label=\"2 (پسرم)\"]\n",
      "3 [label=\"3 (را)\"]\n",
      "3 -> 2 [label=\"PREDEP\"]\n",
      "4 [label=\"4 (به)\"]\n",
      "4 -> 5 [label=\"POSDEP\"]\n",
      "5 [label=\"5 (مدرسه)\"]\n",
      "6 [label=\"6 (بردم)\"]\n",
      "6 -> 1 [label=\"PUNC\"]\n",
      "6 -> 3 [label=\"OBJ\"]\n",
      "6 -> 4 [label=\"VPP\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "complex_sentence = '.پسرم را به مدرسه بردم'\n",
    "complex_graph = parser.parse(word_tokenize(complex_sentence))\n",
    "print(complex_graph.to_dot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(بردم . (را پسرم) (به مدرسه))\n"
     ]
    }
   ],
   "source": [
    "complex_graph.tree().pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same code we used above, but this time we will make it a function so that we can easily get the objects for any sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that takes in a sentence and then returns a \"node\" object for each direct object word\n",
    "def compute_direct_objects(sentence):\n",
    "    dependency_graph = parser.parse(word_tokenize(sentence))\n",
    "    \n",
    "    # we will create a list which we will add all the nodes too, starting with the root\n",
    "    nodes_to_search = [dependency_graph.root]\n",
    "    # we will store the OBJ of the sentence here\n",
    "    object_nodes = []\n",
    "\n",
    "    while len(nodes_to_search) > 0:\n",
    "        # get and remove the first item from the nodes_to_search list\n",
    "        current_node = nodes_to_search.pop()\n",
    "        is_node_object = False\n",
    "        if current_node['rel'] == 'OBJ':\n",
    "            object_nodes.append(current_node)\n",
    "            # now indicate to add any directly connected words\n",
    "            is_node_object = True\n",
    "\n",
    "\n",
    "        # now add this nodes dependencies to the nodes_to_search\n",
    "        for dependency in current_node['deps'].values():\n",
    "            for node_index in dependency:\n",
    "                nodes_to_search.append(dependency_graph.nodes[node_index])\n",
    "                # add the dependent nodes to object_nodes as well if the current node is an object node\n",
    "                if is_node_object:\n",
    "                    object_nodes.append(dependency_graph.nodes[node_index])    \n",
    "    return object_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects:\n",
      "را\n",
      "پسرم\n"
     ]
    }
   ],
   "source": [
    "objects = compute_direct_objects(complex_sentence)\n",
    "print('Objects:')\n",
    "for obj in objects:\n",
    "    print(obj['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is a key concept in NLP as it is one of the first concepts taught that shows how computers can understand not only words, but how words are connected together. For example, \"I showed the teacher my son\" and \"I showed my son the teacher\" have different meanings and dependency parsing is one way we can automatically find this meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "Dependency Parsing uses every concept we know so far, specifically tokenization to separate each word of a sentence, part-of-speech tagging to know what type of word each word is, and then chunking to find the phrases in a sentence. It then finds how these words actually connect.\n",
    "\n",
    "We will now look at a two sentence document. We will break it up into sentence using sentence tokenization, and then we will return the direct objects of each of the sentences using the function we created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['من یک کتاب خوب می\\u200cخوانم.', 'من یک ماشین جدید خریدم.']\n"
     ]
    }
   ],
   "source": [
    "# Prepare an example document with two sentences\n",
    "example_document = '''من یک کتاب خوب می‌خوانم. من یک ماشین جدید خریدم.'''\n",
    "\n",
    "# Prepare the hazm SentenceTokenizer\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(example_document)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "من یک کتاب خوب می‌خوانم.\n",
      "['کتاب', 'یک', 'خوب']\n",
      "من یک ماشین جدید خریدم.\n",
      "['ماشین', 'یک', 'جدید']\n"
     ]
    }
   ],
   "source": [
    "# Now use our `compute_direct_objects` function to find the objects of each sentence.\n",
    "for i in range(len(sentences)):\n",
    "    objects = compute_direct_objects(sentences[i])\n",
    "    print(sentences[i])\n",
    "    print([obj['word'] for obj in objects])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK\n",
    "\n",
    "Now that we have used `hazm` to do these NLP analysis on Persian text, we will learn how to do the exact same operations using `nltk` in case we want to do an analysis on a text written in a different language. The function names are basically the same, so we will quickly go through each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to download some models so `nltk` can do part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/noah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/noah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run part-of-speech tagging, and then show chunking all on an English example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'VBG'), ('with', 'IN'), ('NLTK', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence in English\n",
    "sentence = \"I am learning Natural Language Processing with NLTK\"\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(word_tokenize(sentence))\n",
    "print('POS tags:', pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: (S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  learning/VBG\n",
      "  Natural/NNP\n",
      "  Language/NNP\n",
      "  Processing/VBG\n",
      "  with/IN\n",
      "  NLTK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "print('Chunks:', tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the part of speech tags are different than the ones we saw in `hazm` because the model we downloaded for `nltk` uses the Penn Part of Speech Tags convention. Here \"I\" is a `PRP` which stands for \"personal pronoun\". The list of all part of speech types in the Penn Part of Speech Tags is available at [this NYU webpage](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html).\n",
    "\n",
    "To get all nouns, one needs to find all the parts of speech that are noun types and then see if that words part of speech matches any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['I', 'Natural', 'Language', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "# create a list of all the part of speech tags we want to match\n",
    "all_noun_types = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n",
    "\n",
    "# now go through the tags and add any that are in our list\n",
    "nouns = []\n",
    "for tag in pos_tags:\n",
    "    if tag[1] in all_noun_types:\n",
    "        nouns.append(tag[0])\n",
    "\n",
    "print('Nouns:', nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "The final part of this introduction shows how to do Named Entity Recognition (NER) using the spaCy library. NER finds proper nouns in a sentence so that we can know which people, businesses and other entities are mentioned.\n",
    "\n",
    "NER is a statistical approach meaning that some training or data approach is used so it may not find all entities depending upon the quality of the model used. spaCy does not have a Persian specific model or capability, but does have a multi-language support that can perform some Persian analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use spaCy, we need to either train it ourselves which requires a powerful computer and lots of data, or pick an already trained model. For this lesson, we will use an already trained model, which we will need to first download using the following line. This model is trained on a wide variety of languages, so it can do some Persian but is not fully reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xx-ent-wiki-sm==3.5.0 from https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.5.0/xx_ent_wiki_sm-3.5.0-py3-none-any.whl in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from xx-ent-wiki-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (44.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.1.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.26.15)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('xx_ent_wiki_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load this already trained model into the variable `nlp`. We can then process any sentence by just calling `nlp(sentence)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "doc = nlp(\"محمد در حال شنا است\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computed many values about the sentence, including the entities in it, specifically a persons name. To view the entities, all we need to do is show `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(محمد در,)\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual name text, we can go through each of the labels and get the `.text` attribute. There are some other attributes such as `.label` but those labels are not very useful for Persian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('محمد در', 4317129024397789502)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER is a challenging problem and requires a lot of training data, so the above model will not always find all the names as it is a generic model that is not optimized for Persian. In the next NLP lesson we will learn more about the technology behind this, and what actually happens when we call `nlp(sentence)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have now learned core ideas of NLP that are required for both the most simple methods and the latest advancements in NLP. For example, tokenization is a core part of advanced models such as GPT-4.\n",
    "\n",
    "We now know how to get every noun in a sentence which can allow analysis such as taking a large document and then getting the items mentioned in it. Dependency parsing then allows going a further step and doing things like finding every object of a specific verb.\n",
    "\n",
    "Finally, we covered a basic use of `spaCy` to get named entity recognition which gives a first look at what modern NLP methods can do."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
