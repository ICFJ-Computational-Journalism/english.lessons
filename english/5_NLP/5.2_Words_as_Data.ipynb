{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: persian_sa in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from persian_sa) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from persian_sa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from persian_sa) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn->persian_sa) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn->persian_sa) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn==1.0.1) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install persian_sa\n",
    "!pip3 install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMeQryuJ6CHy"
   },
   "source": [
    "# NLP - Words as Data\n",
    "\n",
    "In the previous lesson we learned the core ideas that enable NLP, and now we will go into some more advanced use cases.\n",
    "\n",
    "Computers cannot understand words unless they are converted into data that the computer can process. There are many ways to do this, such as storing individual words as typed characters, but for some tasks there are other ways to represent words. In this lesson, we will specifically look at storing words as \"vectors\" and being able to compare how similar the meaning of two words is.\n",
    "\n",
    "We will then explore a library that uses NLP to determine the sentiment of a sentence, specifically if it is positive or negative. Sentences with negative sentiments could include reviews that criticize the product or messages that attack a political candidate.\n",
    "\n",
    "Finally, we will explore text generation. This is where a computer can write sentences, paragraphs or even whole documents. This is what tools like ChatGPT can do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the `hazm` library, which we used in the last lesson. We will use the same word tokenizer feature we learned there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n13gAj3g9jO0"
   },
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3O0doK84FNw"
   },
   "source": [
    "### Gensim\n",
    "\n",
    "We will use this library to compare how similar words are together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0pMtIp6E4Dxc"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persian_sa\n",
    "\n",
    "This library provides an easy way to do sentiment analysis on persian text. We will explore this library and explain a little about how sentiment analysis is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import persian_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvRo3eOj6LMQ"
   },
   "source": [
    "## Words as Vectors\n",
    "\n",
    "Computers do not think in concepts or words but rather mathematically operations, so to do advanced NLP tasks we need to be able to convert the problem into a math one. One key idea is representing words not as text, but rather as numbers. This allows us to do all kinds of mathematical operations.\n",
    "\n",
    "One key idea is representing words as \"vectors.\" A vector is a series of numbers, for example, `(1, 2, 3, 4)` is a 4 dimensional vector, meaning it has 4 numbers.\n",
    "\n",
    "We could represent words by having a vector with as many dimensions as there are words, so \"the\" might be `(1, 0, 0, 0, ...)` and \"and\" might be `(0, 1, 0, 0, 0, ...)` and so on. This is how models such as GPT see words.\n",
    "\n",
    "Another way is to use a few dimensions and have each word be placed closest to words that are similar to it. For example, \"chocolate\" and \"sugar\" could represented as `(0.9, 1.0)` and `(1.0, 0.9)` respectively while \"sour\" could be represented by `(-1.0, -1.0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du0rWAs3_Wco"
   },
   "source": [
    "Manually representing would be impossible because of how many words there are, so these models must be \"trained\" and it will learn which words are similar to others and find which numbers best represent each word. This means that they must learn from examples given to them.\n",
    "\n",
    "Typically these will be trained with very large sets of data, but for now, we will be using just a few sentences as it takes a long time to train on large sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "B00JiDVi6IgV"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31j6bkLp68Ea"
   },
   "source": [
    "To train a Word2Vec model, once must first convert the input into a series of words. From the previous notebook we know how to do that.\n",
    "We will load the text and then use `hazm` to convert it into a list of words.\n",
    "\n",
    "For our example, we will have 10 sentences about boats and water, but one can give this any text one wants such as multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z9q8cVY9aJe",
    "outputId": "6a326ca7-59e5-4e65-c7c0-b6f113eb0ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['من', 'دوست', 'دارم', 'با', 'قایق', 'بر', 'روی', 'آب', 'بگردم', '.'], ['این', 'قایق', 'بادبانی', 'بسیار', 'زیبا', 'و', 'رنگین', 'است', '.'], ['آب', 'دریا', 'بسیار', 'شور', 'و', 'آبله', 'دار', 'است', '.'], ['ما', 'باید', 'قبل', 'از', 'قایق', 'سواری', '،', 'جلیقه', 'نجات', 'بپوشیم', '.'], ['این', 'قایق', 'موتوری', 'بسیار', 'سرعت', 'زیادی', 'دارد', '.'], ['آب', 'رودخانه', 'بسیار', 'شفاف', 'و', 'تمیز', 'است', '.'], ['ما', 'با', 'قایق', 'های', 'کاغذی', 'بازی', 'می', 'کنیم', '.'], ['این', 'قایق', 'تفریحی', 'بسیار', 'بزرگ', 'و', 'لوکس', 'است', '.'], ['آب', 'باران', 'بسیار', 'خنک', 'و', 'تازه', 'است', '.'], ['ما', 'با', 'قایق', 'های', 'چوبی', 'ماهی', 'می', 'گیریم', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = hazm.SentenceTokenizer()\n",
    "example_text = \"\"\"من دوست دارم با قایق بر روی آب بگردم.\n",
    "این قایق بادبانی بسیار زیبا و رنگین است.\n",
    "آب دریا بسیار شور و آبله دار است.\n",
    "ما باید قبل از قایق سواری، جلیقه نجات بپوشیم.\n",
    "این قایق موتوری بسیار سرعت زیادی دارد.\n",
    "آب رودخانه بسیار شفاف و تمیز است.\n",
    "ما با قایق های کاغذی بازی می کنیم.\n",
    "این قایق تفریحی بسیار بزرگ و لوکس است.\n",
    "آب باران بسیار خنک و تازه است.\n",
    "ما با قایق های چوبی ماهی می گیریم.\"\"\"\n",
    "example_text = first_examples + second_examples\n",
    "raw_sentences = sentence_tokenizer.tokenize(example_text)\n",
    "examples = [hazm.word_tokenize(sentence) for sentence in raw_sentences]\n",
    "print(examples[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NsfJOP59x8V"
   },
   "source": [
    "Now that we have our data loaded and processed, we can train a simple Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO_gm1oW6lrl",
    "outputId": "3e69dcf5-3a91-4f90-feef-c07d5af54bae"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(examples, min_count=1, vector_size=10, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PkIrDJe7Xvd"
   },
   "source": [
    "Congratulations you have trained your first machine learning model!\n",
    "\n",
    "Above, we can see we are creating a Word2Vec model with our `examples` data. `min_count` allows us to exclude uncommon words that occur less than that value (here we don't exclude any words). `vector_size` says how many dimensions our word vectors should have. `window` instructs it how close words can be to still be considered related. Finally, `sg` is the training algorithm we want to use, where here we select skip gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the numbers that represent each word. We just need to do the following to find how boat is represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07374784, -0.01582357, -0.04507647,  0.06532975, -0.04848628,\n",
       "       -0.01903631,  0.02975632,  0.01096865, -0.08372024, -0.09449303],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['قایق']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how similar sea and salty are compared to river and salty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "GDbLv_mY-X7_",
    "outputId": "1003d6e5-b033-4c69-fed0-bab73263a247",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158655"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('دریا', 'شور')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, they show a positive score which means they share some similitary. Now let us see the result for river and salty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2831113"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('رودخانه', 'شور')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is negative, indicating that river and salty are not related words.\n",
    "\n",
    "These are relatively simple uses, but this technology has many important capabilities. For example, we might want to find where \"operating procedures\" are explained in a long document, but the document might use a different phrasing such as \"methodology\". By using these vectors, we could find this text even though it doesn't match our search words.\n",
    "\n",
    "Tools such as OpenAI's GPT models can also generate these vectors not only for single words, but also for entire documents so one could now do a complex search such as \"laws applying to the sales of boats\" and if all the documents had these vectors created for them, then the documents containing these details would have the highest similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the task of identifying and extracting the emotional polarity (positive, negative, neutral, etc.) of a given text. Sentiment analysis can be useful for various applications, such as social media analysis, customer feedback, product reviews, etc.\n",
    "\n",
    "In this lesson, we will learn how to use a Python library called persian_sa to perform sentiment analysis on Persian textspersian_sa is a machine learning based API that uses a trained model to predict the sentiment class of a given Persian text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use persian_sa, we need to import it and create an instance of the persian_sa class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.linear_model.stochastic_gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpersian_sa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpersian_sa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m persian_sa\n\u001b[1;32m      2\u001b[0m sa \u001b[38;5;241m=\u001b[39m persian_sa()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages/persian_sa/persian_sa.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/svmmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m preprocessing_pipline \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/preprocessingpipline.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msvm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m preprocess_pipeline \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(preprocessing_pipline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mpersian_sa\u001b[39;00m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.linear_model.stochastic_gradient'"
     ]
    }
   ],
   "source": [
    "from persian_sa.persian_sa import persian_sa\n",
    "sa = persian_sa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model.stochastic_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the predict_sentiment method to get the sentiment prediction for any Persian text. The method returns either 'Positive!' or 'Negative!' as the output. Optionally, we can also set the return_class_label argument to True to get the class number instead of the string: 0 for negative and 1 for positive.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'این فیلم بسیار خوب و جذاب بود'\n",
    "sa.predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'این کتاب خیلی بد و خسته کننده است'\n",
    "sa.predict_sentiment(text, return_class_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examples\n",
    "Let’s try some more examples of sentiment analysis with Persian texts. We will use some texts from Ganjoor2, an online collection of Persian poems.\n",
    "\n",
    "#### Example 1: A poem by Hafez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''به بوی نافه‌ای کاخر صبا زان می‌آید\n",
    "که در ره عشق او خاک را زیر می‌کشم\n",
    "\n",
    "به شکرانه نثار آن که جانم فدای اش است\n",
    "زلف چون شسته شود آب حیوان می‌دهم'''\n",
    "sa.predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poem expresses the poet’s love and devotion for his beloved, and his willingness to sacrifice everything for her. The sentiment is positive.\n",
    "\n",
    "#### Example 2: A poem by Saadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''گر هزار دل دارم به هر دل صد غم است\n",
    "وین همه غم نبودی گر نبودی تو کم است\n",
    "\n",
    "گر بخواهی که بمیرم من از آن شادم کن\n",
    "کز توام هرچه برآید همه آن شادم است'''\n",
    "sa.predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poem expresses the poet’s sorrow and pain for his beloved, and his readiness to die for her. The sentiment is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "In simple terms, the `persian_sa` library works by converting words into a vector representation as we learned in the previous part of this lesson. Then it uses what is called a \"Support Vector Machine\" which is a mathematical approach to find a line that best lies between points of the different types. For example, if all negative words had a number that was less than 4.0 and all positive words had that number greater than 4.0, then an SVM would learn that 4.0 was a good point to divide the negative and positive words. These vectors have lots of numbers and no simple rule, so it becomes more complicated than this simple case, but the idea is the same.\n",
    "\n",
    "This approach is machine learning as they took lots of english sentences whose sentiment was known, and then translated them into persian, before then converting them to vectors and then training this Support Vector Machine to learn how to determine which is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most advanced NLP technology we will learn and is what the recent models such as ChatGPT use. In fact, below we will be running a predecessor to ChatGPT called GPT-2 that was trained on persian text. This will be running entirely on ones own computer, so it may be slow to generate text, but for large NLP projects there are servers or more powerful laptops that can be used to perform these tasks quickly.\n",
    "\n",
    "First though, we will need to install some packages. Because these can take some time to finish installing, they were not included in the original set up.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "This is a library that allows us to download and run many already trained models such as the one we are using today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (4.29.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch\n",
    "\n",
    "This is a fully featured machine learning library that one can use to build the latest and most advanced models. Today, we will be using it to run an already trained model, but one can build ones own models and train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: typing-extensions in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: filelock in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: setuptools in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (44.0.0)\n",
      "Requirement already satisfied: wheel in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.34.2)\n",
      "Requirement already satisfied: cmake in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (16.0.5.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to download and load the pre-traineed models. \"Pretrained\" is the word used in NLP when somebody else has already designed and trained the model and one now is only loading and using it. When people use ChatGPT they are using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e361747e24d71b3d7b8f21463ee77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/485M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/gpt2-fa\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HooshvareLab/gpt2-fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is similar to the word tokenizers we have been using from `hazm` but it generates very large vectors that the trained GPT-2 based model can understand.\n",
    "\n",
    "The model will then be responsible for generating text.\n",
    "\n",
    "We will now create a function that will allow us to generate text and specify how much text we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, num_return_sequences=1, do_sample=True):\n",
    "  # encode the prompt\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "  # generate output ids\n",
    "  output_ids = model.generate(input_ids,\n",
    "                              max_length=max_length,\n",
    "                              num_return_sequences=num_return_sequences,\n",
    "                              do_sample=do_sample)\n",
    "  # decode output ids\n",
    "  output_texts = tokenizer.batch_decode(output_ids)\n",
    "  # return output texts\n",
    "  return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to generate some text from a simple prompt. A \"prompt\" is what we call the first words we show the model and the model will then add words after the prompt text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "داستانی درباره یک قهرمان نوشته شده است که به شدت درگیر موضوعات پیچیده تاریخی و فلسفی است تا جایی که بیننده برای خواندن داستان خود می‌خواند و این کار باعث می‌شود که نویسنده بداند که می‌تواند به طور کامل به موضوعات فلسفی برسد\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"داستانی درباره یک قهرمان نوشته شده است که\"\n",
    "output_texts = generate_text(prompt)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one possible text that the model generated from the prompt. It is a fluent and coherent text that introduces a typical hero’s journey story. We can see that the model has learned some common elements of storytelling, such as setting up a goal, a conflict, and a challenge for the protagonist.\n",
    "\n",
    "We can also generate more than one text from the same prompt by changing the `num_return_sequences` parameter. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "داستانی درباره یک قهرمان نوشته شده است که از زندگی‌اش می‌گوید. با وجود اینکه هیچ‌گاه کسی به این موضوع واکنش نشان نداد، اما باز این موضوع در ذهن بسیاری از ما تبدیل شد به پدیده‌ای در میان عاشقان دنیای\n",
      "\n",
      "Text 2:\n",
      "داستانی درباره یک قهرمان نوشته شده است که برای همه یک داستان تعریف شده است و در آن زندگی شخصیت‌ها و اتفاقات به شکل دیگری روایت می‌شود. «مارتا کاتبیو» فیلمی درام است که در سال ۲۰۱۸ اکران شد.\n",
      "\n",
      "Text 3:\n",
      "داستانی درباره یک قهرمان نوشته شده است که از طرف مجله نیویورک‌تایمز منتشر می‌شود. او یک وکیل است و مدتی قبل به جرم قتل همسرش در خانه مجرم شناخته شد. این فیلم در جشنواره‌های مختلف از جمله کن و نیز در\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"داستانی درباره یک قهرمان نوشته شده است که\"\n",
    "output_texts = generate_text(prompt, num_return_sequences=3)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, each time it can generate very different stories. I recommend trying ones own \"prompt\" by editing the sentence and seeing what it generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the max_length parameter to generate longer or shorter texts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: قایق‌ها عموما\n",
      "\n",
      "Text 2:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: بله ما در مسیر\n",
      "\n",
      "Text 3:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: بله! بله!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ:\"\n",
    "output_texts = generate_text(prompt, num_return_sequences=3, max_length=20)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the answers may not be true or accurate, but with a more advanced model, this could be used to fill out information.\n",
    "\n",
    "For example, one could take every sentence in a document, and provide the model each sentence in the form, `Does the sentence \"This is a great company to get coverage for ones car.\" mention an insurance business?` and then if the model outputs \"yes\" in its answer, one could classify this. This could be a way to find every mention of insurance businesses in a document for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technology has many possibilities from generating drafts of news stories to answering questions about text and documents.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have now learned many advanced use cases of NLP including running an older version of the machine learning models that power ChatGPT."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
