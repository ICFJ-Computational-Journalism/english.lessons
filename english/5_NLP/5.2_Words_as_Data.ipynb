{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMeQryuJ6CHy"
   },
   "source": [
    "# NLP - Words as Data\n",
    "\n",
    "In the previous lesson we learned the core ideas that enable NLP, and now we will go into some more advanced use cases.\n",
    "\n",
    "Computers cannot understand words unless they are converted into data that the computer can process. There are many ways to do this, such as storing individual words as typed characters, but for some tasks there are other ways to represent words. In this lesson, we will specifically look at storing words as \"vectors\" and being able to compare how similar the meaning of two words is. Another word for these vectors is \"embeddings\" as they take the meaning of a word and then \"embed\" it in numbers.\n",
    "\n",
    "We will then explore a library that uses NLP to determine the sentiment of a sentence, specifically if it is positive or negative. Sentences with negative sentiments could include reviews that criticize a product or messages that attack a political candidate.\n",
    "\n",
    "Finally, we will explore text generation. This is where a computer can write sentences, paragraphs or even whole documents. This is what ChatGPT does. We will even run a persian trained GPT-2 model directly on our laptops to see how these models work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazm\n",
    "\n",
    "First we will import the `hazm` library, which we used in the last lesson. We will use the same word tokenizer feature we learned there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n13gAj3g9jO0"
   },
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3O0doK84FNw"
   },
   "source": [
    "### Gensim\n",
    "\n",
    "We will use this library to compare how similar words are together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0pMtIp6E4Dxc"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "This useful library is covered in earlier NLP lessons. We will use it to store and process some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "This package provides a bunch of traditional machine learning methods such as being able to fit a line through data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch and transformers\n",
    "\n",
    "We will download and import these two libraries later. These are used for the latest cutting edge NLP technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvRo3eOj6LMQ"
   },
   "source": [
    "## Words as Vectors\n",
    "\n",
    "Computers do not think in concepts or words but rather mathematically operations, so to do advanced NLP tasks we need to be able to convert the problem into a math one. One key idea is representing words not as text, but rather as numbers. This allows us to do all kinds of mathematical operations.\n",
    "\n",
    "One key idea is representing words as \"vectors.\" A vector is a series of numbers, for example, `(1, 2, 3, 4)` is a 4 dimensional vector, meaning it has 4 numbers.\n",
    "\n",
    "We could represent words by having a vector with as many dimensions as there are words, so \"the\" might be `(1, 0, 0, 0, ...)` and \"and\" might be `(0, 1, 0, 0, 0, ...)` and so on. This is how models such as GPT see words.\n",
    "\n",
    "Another way is to use a few dimensions and have each word be placed closest to words that are similar to it. For example, \"chocolate\" and \"sugar\" could represented as `(0.9, 1.0)` and `(1.0, 0.9)` respectively while \"sour\" could be represented by `(-1.0, -1.0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du0rWAs3_Wco"
   },
   "source": [
    "Manually representing would be impossible because of how many words there are, so these models must be \"trained\" and it will learn which words are similar to others and find which numbers best represent each word. This means that they must learn from examples given to them.\n",
    "\n",
    "Typically these will be trained with very large sets of data, but for now, we will be using just a few sentences as it takes a long time to train on large sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B00JiDVi6IgV"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31j6bkLp68Ea"
   },
   "source": [
    "To train a Word2Vec model, once must first convert the input into a series of words. From the previous notebook we know how to do that.\n",
    "We will load the text and then use `hazm` to convert it into a list of words.\n",
    "\n",
    "For our example, we will have 10 sentences about boats and water, but one can give this any text one wants such as multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z9q8cVY9aJe",
    "outputId": "6a326ca7-59e5-4e65-c7c0-b6f113eb0ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['من', 'دوست', 'دارم', 'با', 'قایق', 'بر', 'روی', 'آب', 'بگردم', '.'], ['این', 'قایق', 'بادبانی', 'بسیار', 'زیبا', 'و', 'رنگین', 'است', '.'], ['آب', 'دریا', 'بسیار', 'شور', 'و', 'آبله', 'دار', 'است', '.'], ['ما', 'باید', 'قبل', 'از', 'قایق', 'سواری', '،', 'جلیقه', 'نجات', 'بپوشیم', '.'], ['این', 'قایق', 'موتوری', 'بسیار', 'سرعت', 'زیادی', 'دارد', '.'], ['آب', 'رودخانه', 'بسیار', 'شفاف', 'و', 'تمیز', 'است', '.'], ['ما', 'با', 'قایق', 'های', 'کاغذی', 'بازی', 'می', 'کنیم', '.'], ['این', 'قایق', 'تفریحی', 'بسیار', 'بزرگ', 'و', 'لوکس', 'است', '.'], ['آب', 'باران', 'بسیار', 'خنک', 'و', 'تازه', 'است', '.'], ['ما', 'با', 'قایق', 'های', 'چوبی', 'ماهی', 'می', 'گیریم', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = hazm.SentenceTokenizer()\n",
    "example_text = \"\"\"من دوست دارم با قایق بر روی آب بگردم.\n",
    "این قایق بادبانی بسیار زیبا و رنگین است.\n",
    "آب دریا بسیار شور و آبله دار است.\n",
    "ما باید قبل از قایق سواری، جلیقه نجات بپوشیم.\n",
    "این قایق موتوری بسیار سرعت زیادی دارد.\n",
    "آب رودخانه بسیار شفاف و تمیز است.\n",
    "ما با قایق های کاغذی بازی می کنیم.\n",
    "این قایق تفریحی بسیار بزرگ و لوکس است.\n",
    "آب باران بسیار خنک و تازه است.\n",
    "ما با قایق های چوبی ماهی می گیریم.\"\"\"\n",
    "raw_sentences = sentence_tokenizer.tokenize(example_text)\n",
    "examples = [hazm.word_tokenize(sentence) for sentence in raw_sentences]\n",
    "print(examples[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NsfJOP59x8V"
   },
   "source": [
    "Now that we have our data loaded and processed, we can train a simple Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO_gm1oW6lrl",
    "outputId": "3e69dcf5-3a91-4f90-feef-c07d5af54bae"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(examples, min_count=1, vector_size=10, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PkIrDJe7Xvd"
   },
   "source": [
    "Congratulations you have trained your first machine learning model!\n",
    "\n",
    "Above, we can see we are creating a Word2Vec model with our `examples` data. `min_count` allows us to exclude uncommon words that occur less than that value (here we don't exclude any words). `vector_size` says how many dimensions our word vectors should have. `window` instructs it how close words can be to still be considered related. Finally, `sg` is the training algorithm we want to use, where here we select skip gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the numbers that represent each word. We just need to do the following to find how boat is represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07374784, -0.01582357, -0.04507647,  0.06532975, -0.04848628,\n",
       "       -0.01903631,  0.02975632,  0.01096865, -0.08372024, -0.09449303],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['قایق']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how similar sea and salty are compared to river and salty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "GDbLv_mY-X7_",
    "outputId": "1003d6e5-b033-4c69-fed0-bab73263a247",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158655"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('دریا', 'شور')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, they show a positive score which means they share some similitary. Now let us see the result for river and salty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2831113"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('رودخانه', 'شور')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is negative, indicating that river and salty are not related words.\n",
    "\n",
    "These are relatively simple uses, but this technology has many important capabilities. For example, we might want to find where \"operating procedures\" are explained in a long document, but the document might use a different phrasing such as \"methodology\". By using these vectors, we could find this text even though it doesn't match our search words.\n",
    "\n",
    "Tools such as OpenAI's GPT models can also generate these vectors not only for single words, but also for entire documents so one could now do a complex search such as \"laws applying to the sales of boats\" and if all the documents had these vectors created for them, then the documents containing these details would have the highest similarity.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "With all NLP models, if it has never seen a word before in its training, it will not be able to process it. So if you want to use a specific word, make sure it is in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the task of identifying and extracting the emotional polarity (positive or negative) of a given text. Sentiment analysis can be useful for various applications, such as social media analysis, customer feedback, product reviews, etc.\n",
    "\n",
    "There are advanced models, but we will use the Word2Vec tool we used above to create a simple approach that can identify positive or negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_text = \"من امروز خیلی خوشحالم چون کارم را تمام کردم. او به من گل زیبایی هدیه داد و من را شاد کرد. ما با دوستانمان به پارک رفتیم و خیلی لذت بردیم. این فیلم بسیار جالب و خنده دار بود. من از موفقیت شما خوشحالم. او با لبخند گفت: من عاشق تو هستم. مادرم برای من غذای مورد علاقه ام درست کرد. این کتاب بسیار جذاب و آموزنده بود. ما در قرعه کشی یک سفر رایگان برنده شدیم. این آهنگ بسیار شاد و شنیدنی است.\"\n",
    "negative_text = \"من امروز خیلی ناراحتم چون کارم را از دست دادم. او به من گفت که من بی استعداد و بی ارزش هستم. ما با دوستانمان به سینما رفتیم ولی فیلم بسیار بد و خسته کننده بود. این کتاب بسیار خشک و بی معنی بود. من از شکست شما لذت می برم. او با تنفر گفت: من از تو متنفرم. مادرم به من گفت که من هیچ وقت نمی توانم به آرزوهایم برسم. این آهنگ بسیار زشت و ناهنجار است. ما در قرعه کشی یک جایزه بزرگ از دست دادیم. این غذا بسیار تلخ و ترش است.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will prepare these sentences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = hazm.SentenceTokenizer()\n",
    "positive_sentences = sentence_tokenizer.tokenize(positive_text)\n",
    "negative_sentences = sentence_tokenizer.tokenize(negative_text)\n",
    "raw_sentences = positive_sentences + negative_sentences\n",
    "examples = [hazm.word_tokenize(sentence) for sentence in raw_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(examples, min_count=1, vector_size=5, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function that can take individual word vectors and create a single vector for an entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vector(text):\n",
    "  # tokenize the text\n",
    "  tokens = hazm.word_tokenize(text)\n",
    "  # filter out punctuation and non-alphabetic tokens\n",
    "  tokens = [token for token in tokens if token.isalpha()]\n",
    "  # get the vectors of the tokens\n",
    "  vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "  # return the average vector or a zero vector if empty\n",
    "  return np.mean(vectors, axis=0) if vectors else np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up and train a simple model. It is called logistic regression and is a simple and common way to learn how best to split two groups apart, in this case positive and negative sentences.\n",
    "\n",
    "For the model, we will have a `train_vectors` array which will be all the sentences we will use to train, and then a `y_train` which will specify the sentiment with 0 being negative and 1 being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.concatenate((\n",
    "    np.array([get_average_vector(sentence) for sentence in positive_sentences]),\n",
    "    np.array([get_average_vector(sentence) for sentence in negative_sentences])))\n",
    "y_train = np.concatenate((np.ones(len(positive_sentences)), np.zeros(len(negative_sentences))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a function that will print \"Positive\" if it thinks the text is positive, and \"Negative\" if it thinks the text is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # this converts the text to a vector and then shows it to the model\n",
    "    y_pred = clf.predict([get_average_vector(text)])\n",
    "    if y_pred < 0.5:\n",
    "        print(\"Positive!\")\n",
    "    else:\n",
    "        print(\"Negative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will see some examples of our model working to predict positive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive!\n"
     ]
    }
   ],
   "source": [
    "# Example positive sentence\n",
    "predict(\"من از موفقیت شما خوشحالم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive!\n"
     ]
    }
   ],
   "source": [
    "# Example positive sentence\n",
    "predict(\"من با لبخند گفتم: من از شما خیلی خوشحالم..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for some examples of negative text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative!\n"
     ]
    }
   ],
   "source": [
    "# Example negative sentence\n",
    "predict(\"او به من گفت که من بی استعداد و بی ارزش هستم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative!\n"
     ]
    }
   ],
   "source": [
    "# Example negative sentence\n",
    "predict(\"مادرم به من گفت که من بی معنی و بی ارزش هستم.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not work perfectly though and may make mistakes. For example, the below negative sentence is misclassified as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive!\n"
     ]
    }
   ],
   "source": [
    "# Example negative sentence which is incorrectly classified\n",
    "predict(\"من از کارم خسته شدم و هیچ لذتی نبردم.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Conclusions\n",
    "\n",
    "We have now taken the ability to generate word vectors and used this to build and train a simple model for sentiment analysis. One key lesson here is how powerful representing words as numbers can be and it lets us use all kinds of analysis and statistics tools such as logistic regression to develop models.\n",
    "\n",
    "There are many more \"classification\" tasks in NLP than just sentiment analysis. For example, we might want to classify formal and informal phrasing.\n",
    "\n",
    "No matter the specific classification task it will use the same core flow we have now learned:\n",
    "1. Tokenize the text into individual words or groups of letters\n",
    "2. Convert these into a number representation which can be done by training a Word2Vec model\n",
    "3. Train a model on this number representation. This can be as simple as a statistics model or as complex as advanced systems like ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most advanced NLP technology we will learn and is what the recent models such as ChatGPT use. In fact, below we will be running a predecessor to ChatGPT called GPT-2 that was trained on persian text. This will be running entirely on ones own computer, so it may be slow to generate text, but for large NLP projects there are servers or more powerful laptops that can be used to perform these tasks quickly.\n",
    "\n",
    "First though, we will need to install some packages. Because these can take some time to finish installing, they were not included in the original set up.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "This is a library that allows us to download and run many already trained models such as the one we are using today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch\n",
    "\n",
    "This is a fully featured machine learning library that one can use to build the latest and most advanced models. Today, we will be using it to run an already trained model, but one can build ones own models and train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: typing-extensions in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: filelock in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: setuptools in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (44.0.0)\n",
      "Requirement already satisfied: wheel in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.34.2)\n",
      "Requirement already satisfied: cmake in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (16.0.5.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/noah/.local/share/virtualenvs/data-journalism-lessons-JftrASKr/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to download and load the pre-traineed models. \"Pretrained\" is the word used in NLP when somebody else has already designed and trained the model and one now is only loading and using it. When people use ChatGPT they are using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e361747e24d71b3d7b8f21463ee77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/485M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/gpt2-fa\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HooshvareLab/gpt2-fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is similar to the word tokenizers we have been using from `hazm` but it generates very large vectors that the trained GPT-2 based model can understand.\n",
    "\n",
    "The model will then be responsible for generating text.\n",
    "\n",
    "We will now create a function that will allow us to generate text and specify how much text we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, num_return_sequences=1, do_sample=True):\n",
    "  # encode the prompt\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "  # generate output ids\n",
    "  output_ids = model.generate(input_ids,\n",
    "                              max_length=max_length,\n",
    "                              num_return_sequences=num_return_sequences,\n",
    "                              do_sample=do_sample)\n",
    "  # decode output ids\n",
    "  output_texts = tokenizer.batch_decode(output_ids)\n",
    "  # return output texts\n",
    "  return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to generate some text from a simple prompt. A \"prompt\" is what we call the first words we show the model and the model will then add words after the prompt text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "داستانی درباره یک قهرمان نوشته شده است که به شدت درگیر موضوعات پیچیده تاریخی و فلسفی است تا جایی که بیننده برای خواندن داستان خود می‌خواند و این کار باعث می‌شود که نویسنده بداند که می‌تواند به طور کامل به موضوعات فلسفی برسد\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"داستانی درباره یک قهرمان نوشته شده است که\"\n",
    "output_texts = generate_text(prompt)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one possible text that the model generated from the prompt. It is a fluent and coherent text that introduces a typical hero’s journey story. We can see that the model has learned some common elements of storytelling, such as setting up a goal, a conflict, and a challenge for the protagonist.\n",
    "\n",
    "We can also generate more than one text from the same prompt by changing the `num_return_sequences` parameter. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "داستانی درباره یک قهرمان نوشته شده است که از زندگی‌اش می‌گوید. با وجود اینکه هیچ‌گاه کسی به این موضوع واکنش نشان نداد، اما باز این موضوع در ذهن بسیاری از ما تبدیل شد به پدیده‌ای در میان عاشقان دنیای\n",
      "\n",
      "Text 2:\n",
      "داستانی درباره یک قهرمان نوشته شده است که برای همه یک داستان تعریف شده است و در آن زندگی شخصیت‌ها و اتفاقات به شکل دیگری روایت می‌شود. «مارتا کاتبیو» فیلمی درام است که در سال ۲۰۱۸ اکران شد.\n",
      "\n",
      "Text 3:\n",
      "داستانی درباره یک قهرمان نوشته شده است که از طرف مجله نیویورک‌تایمز منتشر می‌شود. او یک وکیل است و مدتی قبل به جرم قتل همسرش در خانه مجرم شناخته شد. این فیلم در جشنواره‌های مختلف از جمله کن و نیز در\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"داستانی درباره یک قهرمان نوشته شده است که\"\n",
    "output_texts = generate_text(prompt, num_return_sequences=3)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, each time it can generate very different stories. I recommend trying ones own \"prompt\" by editing the sentence and seeing what it generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the max_length parameter to generate longer or shorter texts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: قایق‌ها عموما\n",
      "\n",
      "Text 2:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: بله ما در مسیر\n",
      "\n",
      "Text 3:\n",
      "سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ: بله! بله!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"سوال: آیا قایق ها می توانند پرواز کنند؟ بله یا خیر. پاسخ:\"\n",
    "output_texts = generate_text(prompt, num_return_sequences=3, max_length=20)\n",
    "for i, text in enumerate(output_texts):\n",
    "  print(f\"Text {i+1}:\")\n",
    "  print(text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the answers may not be true or accurate, but with a more advanced model, this could be used to fill out information.\n",
    "\n",
    "For example, one could take every sentence in a document, and provide the model each sentence in the form, `Does the sentence \"This is a great company to get coverage for ones car.\" mention an insurance business?` and then if the model outputs \"yes\" in its answer, one could classify this. This could be a way to find every mention of insurance businesses in a document for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technology has many possibilities from generating drafts of news stories to answering questions about text and documents.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have now learned many advanced use cases of NLP including running an older version of the machine learning models that power ChatGPT. All modern NLP relies on converting human text into numbers called vectors or embeddings and we learned how we can train our own model to find vectors we can use to compare the similarity of words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
