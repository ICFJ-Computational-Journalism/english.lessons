{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC-B66girPR5"
   },
   "source": [
    "# Introduction to data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8-zoAzBrrx0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMD90ZgXrwfW"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulAWjVGunO6Z"
   },
   "source": [
    "Welcome to our introduction to cleaning data! Real-world data is messy. For example, business data is often entered manually by humans. Humans can be somewhat inconsistent in the way that they enter the data. For example, when writing company names, at times they might write `Apple, Inc.`, and at other times they might just write `Apple`. If we are trying to find the number of times `Apple` is included in a dataset and only consider the first possibility—`Apple, Inc.`—we will not obtain a proper count of the number of occurrences of `Apple`.\n",
    "\n",
    "Luckily, we can take some steps to clean up datasets. In this lesson, we will go through a synthetic dataset of companies that have been selected for a recent regulatory investigation. For each industry represented in the dataset, we will seek to obtain the average tax revenue produced for the companies within that industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFCz_XJGr2iE"
   },
   "source": [
    "## Regulatory Investigation: Tax Revenue Analysis\n",
    "\n",
    "You will get your first taste for data cleaning by seeing it in action. There are some core concepts of data cleaning that we will lay out later on, but data cleaning ultimately relies on an intimate and careful understanding of the data we are working with. By looking at our data and thinking about what we want to do with it, we can determine how we have to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L49VAa8mbO8L"
   },
   "source": [
    "First, we load the dataset from the file and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aQd1Fo70apTW",
    "outputId": "eb4b1200-ee8a-4adc-9c5e-c77204dc6921"
   },
   "outputs": [],
   "source": [
    "investigation_companies = pd.read_csv('../../input/investigation_iran_companies.csv', index_col = [0])\n",
    "display(investigation_companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gA-jbCkbW5j"
   },
   "source": [
    "We notice that some companies appear more than once in the dataset, and they appear under slightly different names. For example, we see that row 18 and row 44 both correspond to Arta Energy, and row 3 and row 29 both correspond to Amin Pharmaceutical. We need to eliminate duplicate entries such as these so that we can get the proper value for the average tax revenue corresponding to each industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTpGRIDsW7pz"
   },
   "source": [
    "#### Dropping Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MdTI0QKcN-h"
   },
   "source": [
    "Since this dataset is quite small, we could manually go through it to remove duplicates. However, we will seek to remove duplicates in an automated manner so that you will have the skills to work with larger datasets. \n",
    "\n",
    "As a first step, we can simply take care of all instances where the exact same company name occurs more than once in the dataset. We can do this using the `drop_duplicates` function in `pandas` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aQiHsVNic_sB",
    "outputId": "65b0d019-9395-428a-ddaa-d3c81a1f43b2"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies.drop_duplicates(subset = 'company_name')\n",
    "display(investigation_companies_dropped)\n",
    "len(investigation_companies_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIZOZ4NXc_fS"
   },
   "source": [
    "We see that there are now 45 rows in our dataset. However, we see that there still some duplicates in the dataset, such as Tehran Pharmaceuticals. We now need to find a way to standardize company names. In other words, we need to make sure that whenever two rows correspond to the same company, there is a `company_id` that is the same between those two rows, even when the `company_name` is different. We could make the `company_id` simply the lower case version of the `company_name`. This would take care of the Arta Energy case, where we have a row for Arta Energy and a row for arta energy. However, in other cases, this would not be enough. For example, rows 15 and 41 refer to Persian Gulf Steel, but row 15 would become \"persian gulf steel & co.\" and row 41 would become \"persian gulf steel.\"\n",
    "\n",
    "What if we can use the `ceo_name` to determine when two companies are in fact the same? This relies on the assumption that no two companies have a CEO with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "brcbkftWgH0k",
    "outputId": "23e171d7-b898-4751-f797-6e63e98053df"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies.drop_duplicates(subset = 'ceo_name')\n",
    "display(investigation_companies_dropped)\n",
    "print(len(investigation_companies_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc2XmFyOgdFA"
   },
   "source": [
    "This led to the removal of just two rows. This could be due to the fact that some of the CEO names are lower-cased. We create a new column with the lower-cased version of the CEO name and see if we can use this as a basis for removing duplicates; this way we might capture more duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52o2mR6Qgwix"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies\n",
    "investigation_companies_dropped['ceo_name_lower'] = investigation_companies_dropped['ceo_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZuZucbLtgwdj",
    "outputId": "15d5b3a9-fba6-419d-fca7-1533442270ce"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies_dropped.drop_duplicates(subset = 'ceo_name_lower')\n",
    "display(investigation_companies_dropped)\n",
    "print(len(investigation_companies_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irEnkwbmhFvk"
   },
   "source": [
    "We see that no additional duplicates are being dropped. What is going here? It appears that the first and last names of the CEOs are getting switched at certain points. For example, for one entry of Persian Gulf Steel & Co., the CEO is abdollah azimi, and for the other it is azimi abdollah. We need to make sure the two names (first and last) of the CEO appear in the same order each time the name appears. To acccomplish this, we can write a function that puts the two names in alphabetical order. This way, for each instance where the two names appear, the two names will appear in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBdv-Q62hnl1"
   },
   "outputs": [],
   "source": [
    "def put_names_in_order(ceo_name):\n",
    "  name_list = ceo_name.split() #make the two names of the CEO into a list\n",
    "  name_list.sort() #Put the names in order\n",
    "  return ' '.join(name_list) #Put the names back together as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGQSYMj8hjBQ"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies\n",
    "investigation_companies_dropped['ceo_name_lower_ordered'] = investigation_companies_dropped.apply(lambda row: put_names_in_order(row['ceo_name_lower']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBEiqX-yihVy"
   },
   "source": [
    "Hopefully, we've now standardized the CEO names. Let's see if we can now use `ceo_name_lower_ordered` as a basis for dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SNSuDdBghFij",
    "outputId": "9ed4e6be-43e1-4ea5-a469-488e80c9e83f"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped = investigation_companies_dropped.drop_duplicates(subset = 'ceo_name_lower_ordered')\n",
    "display(investigation_companies_dropped)\n",
    "print(len(investigation_companies_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZShXoDLkpde"
   },
   "source": [
    "We have now removed many more rows, and we see that we are down to 27 rows! We have made major headway in removing duplicate companies from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXFNwBJiXDqS"
   },
   "source": [
    "#### Data Type Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swEJN6dOk9VG"
   },
   "source": [
    "We now move on to calculating the average tax revenue for the Insurance industry within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "U7_tl5fTlVgv",
    "outputId": "fb1b5c63-a965-4a33-c0a6-cd579a4b3789"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped[investigation_companies_dropped['sector'] == 'Insurance']['tax_revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCljhoVkldKJ"
   },
   "source": [
    "We run into an error here: \n",
    "\n",
    "> TypeError: Could not convert $30000000 to numeric\n",
    "\n",
    "This error relates to a key element of data cleaning: making sure our data types are correct. The issue is that the tax_revenue column is of type str (String). To compute the mean of that column, we need to convert the values in it to numeric values. To accomplish this, we need to follow two steps:\n",
    "1. Remove the $ sign from each string.\n",
    "\n",
    "2. Once we've removed the $ sign, Python is able to convert the string into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JnJY1QTCsrk",
    "outputId": "8f47271e-7096-4f3e-cb1e-d1fb82883209"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped['tax_revenue'] = investigation_companies_dropped['tax_revenue'].apply(lambda row: row.replace(\"$\",\"\"))\n",
    "investigation_companies_dropped['tax_revenue'] = investigation_companies_dropped['tax_revenue'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n4DWlBqyHjt"
   },
   "source": [
    "Now we see that the tax revenue column no longer has $ signs and has data type integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_ba9fmZx85N",
    "outputId": "204fe46b-5674-4a16-ba16-8c1e4998f183"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped['tax_revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z53eyAjpyUj_"
   },
   "source": [
    "And now we can calculate the desired mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZPywiyxyM-Z",
    "outputId": "6d3d5bca-4435-4a48-9703-1c732bc4da24"
   },
   "outputs": [],
   "source": [
    "investigation_companies_dropped[investigation_companies_dropped['sector'] == 'Insurance']['tax_revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXY2SXflXIK0"
   },
   "source": [
    "#### Advanced Topic: Fuzzy Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyprK4QlXOMU"
   },
   "source": [
    "Fuzzy matching is a valuable technique for tackling advanced data cleaning problems, especially when dealing with messy or inconsistent data. Fuzzy matching allows you to find approximate matches between strings, even when there are variations in spelling, formatting, or other discrepancies.\n",
    "\n",
    "One popular Python package for fuzzy matching is called `fuzzywuzzy`. It provides various functions and algorithms for fuzzy string matching. You can use it to compare strings and determine their similarity scores.\n",
    "\n",
    "To get started with `fuzzywuzzy`, you can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9JJt9K0XNkP",
    "outputId": "5820b5ca-5603-43bb-8426-6537b59075d6"
   },
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7AmbHyTXWfB"
   },
   "source": [
    "Once installed, you can import the package and start using its functions in your code. Here's an example of how you can apply fuzzy matching using `fuzzywuzzy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f92qjEFgXUvV",
    "outputId": "4b8a0161-8032-46ef-f85f-1c11883cb52a"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Compare two strings and get their similarity score\n",
    "string1 = \"apple\"\n",
    "string2 = \"apples\"\n",
    "similarity_score = fuzz.ratio(string1, string2)\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMoR9e7LXdnJ"
   },
   "source": [
    "The above will output a similarity score between 0 and 100, indicating the degree of similarity between the two strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j66Kqe0ysN1"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope you enjoyed this lesson on data cleaning. If you clean your data, you will be able to use it to discover fascinating stories."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
