{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba9b27c",
   "metadata": {},
   "source": [
    "## ۵.۱ مقدمه‌ای بر اِن‌ال‌پی (پردازش زبان‌های طبیعی)\n",
    "\n",
    "### اِن‌ال‌پی چیست؟\n",
    "\n",
    "پردازش زبان‌های طبیعی (اِن‌ال‌پی) رشته‌ای در علوم کامپیوتری است که بر تعامل میان زبان‌های بشری و کامپیوتر تمرکز دارد. ان‌ال‌پی این امکان را به کامپیوتر می‌دهد تا زبان انسانی را بفهمد، تفسیر و تولید کند. در روزنامه‌نگاری نیز ان‌ال‌پی بسیار به کار می‌آید؛ از تحلیل پیچیده متون تا تولید بخش‌هایی از یک مقاله. در این نوت‌بوک، مفاهیم هسته‌ای ان‌ال‌پی را بررسی می‌کنیم و از یکی از روش‌های پیشرفته ان‌ال‌پی تحت عنوان «تشخیص موجودیت‌های نامدار» یا همان اِن‌ای‌آر\n",
    "\n",
    "Named Entity Recognition\n",
    "\n",
    "استفاده می‌کنیم.\n",
    "\n",
    "**نصب و پکیج‌ها:** در حال حاضر چندین پکیج ان‌ال‌پی کلیدی وجود دارد که کاربردهای بسیاری دارند و آشنایی با آن‌ها حائز اهمیت است. در این نوت‌بوک نگاهی کلی به این پکیج‌ها می‌اندازیم و در مرحله بعدی از آن‌ها استفاده می‌کنیم.\n",
    "\n",
    "**توکنیزه کردن:** این قدم مهم که برای بسیاری از الگوریم‌ها اساسی است، به این معنای شکستن یک متن به قطعات کوچکتر، به عنوان مثال کلمات است.\n",
    "\n",
    "**برچسب‌گذاری جزء کلام (پی‌او‌اِس):** دستور زبان بخش هسته‌ای هر زبانی است و شناسایی فعل، فاعل و مفعول در جمله نیز از بخش‌های اساسی درک هر زبانی است\n",
    "\n",
    "**تجزیه بر اساس وابستگی:** \n",
    "تجزیه بر اساس وابستگی یا همان \n",
    "\n",
    "Dependency Parsing\n",
    "\n",
    "چیزی شبیه به برچسب‌گذاری جزء کلام یا همان پی‌او‌اِس است، با این تفاوت که به جای تشخیص جایگاه هر کلمه در جمله، ارتباط میان کلمات را پیدا می‌کند. به عنوان مثال، ما می‌توانیم با استفاده از این روش بفهمیم که کدام بخش جمله وابسته به فعل است. مثلا در جمله «پسر توپ را شوت می‌کند» مفعول مستقیم فعل «شوت کردن»، «توپ» است.\n",
    "\n",
    "**اِن‌ای‌آر:** این‌که اسامی چه کسانی در یک متن آمده، می‌تواند کارآیی بسیار داشته باشد. در این نوت‌بوک نشان می‌دهیم که چطور می‌توان اسامی افراد و شرکت‌ها را از متن بیرون بکشیم.\n",
    "\n",
    "در نوت‌بوک بعدی این مفاهیم را بررسی خواهیم کرد: عرضه کلمات در قالب بًردار، عقیده‌کاوی، تشابه معنایی و تولید متن.\n",
    "\n",
    "## نصب و پکیج‌ها\n",
    "\n",
    "در این نوت‌بوک روش استفاده از سه پکیج کلیدی ان‌ال‌پی را می‌آموزیم: اِن‌اِل‌تی‌کِی، هضم و اسپِیسی.\n",
    "\n",
    "### NLTK (Natural Language Toolkit) \n",
    "اِن‌اِل‌تی‌کِی یا همان «ابزار زبان طبیعی» یکی از پکیج‌های ان‌ال‌پی پایتون است که به طرز گسترده‌ای استفاده می‌شود. این پکیج، برای عملیاتی کردن فرمان‌های متداول ان‌ال‌پی کارآیی بسیار دارد. این پکیج برای زبان فارسی به طور ویژه طراحی نشده است ولی بسیاری از ابزارها از آن استفاده می‌کنند و برای اجرای عملیات به زبان انگلیسی بسیار مناسب است.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfee2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d35b3",
   "metadata": {},
   "source": [
    "### HAZM\n",
    "\n",
    "برای تحلیل ان‌ال‌پی به زبان فارسی، کتابخانه هضم الگوریتم‌های اساسی بسیاری را برای متون فارسی در اختیار قرار می‌دهد. هضم با ان‌ال‌تی‌کی هم همساز و همجور است. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71525ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42dd12c",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "اسپیسی یک کتابخانه موثر و مدرن است که در بسیاری از اپلیکیشن‌های تولیدی مورد استفاده قرار می‌گیرد. از اسپیسی هم می‌توان برای تحلیل‌های کوچک استفاده کرد و هم برای ساختن یک سیستم ان‌ال‌پی پیچیده به منظور بررسی سریع هزاران متن. این کتابخانه در حد ابتدایی با زبان فارسی همخوانی دارد. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a4f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16299b75",
   "metadata": {},
   "source": [
    "### توکنیزه کردن\n",
    "\n",
    "یکی از پایه‌های ان‌ال‌پی توکنیزه کردن است. توکنیزه کردن به معنای تقسیم کردن متن به کلمات مجزا، عبارات یا نمادهایی است که به آن‌ها توکن می‌گوییم. توکنیزه کردن نخستین گام در کار با ان‌ال‌پی است و به ما این امکان را می‌دهد تا یک متن را به صورت جزء به جزء تحلیل کنیم. همان‌طور که انسان‌ها از طریق درک مجزای کلمات می‌توانند مکالمات را تحلیل کنند، کامپیوترها هم قادرند جملات و مکالمات را بر اساس کلمات فردی تحلیل کنند.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5018d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['این', 'یک', 'جمله', 'نمونه', 'است', '.']\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"این یک جمله نمونه است.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299799b",
   "metadata": {},
   "source": [
    "همان‌طور که می‌بینیم، این جمله به کلمات حاضر در آن تقسیم شده است. حال می‌توانیم این جمله را تحلیل کنیم و مثلا تعداد کلمات آن را بیابیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9460a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Show word count\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0dc95",
   "metadata": {},
   "source": [
    "### توکنیزه کردن جمله\n",
    "\n",
    "گاهی اوقات می‌خواهیم یک متن را جمله به جمله تحلیل کنیم. برای انجام این کار، باید متن را بر اساس جملات تقسیم کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931ab44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام دنیا!', 'این یک متن فارسی است.', 'امیدوارم که این درس مفید باشد.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentence tokenizer\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"سلام دنیا! این یک متن فارسی است. امیدوارم که این درس مفید باشد.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Print the sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591870c",
   "metadata": {},
   "source": [
    "حال به عنوان تمرین، بیایید ببینیم توکنیزه کردن هر کلمه در این جملات به چه صورت است. روش اول، استفاده از لوپ «فور» یا همان\n",
    "\n",
    "`for`\n",
    "\n",
    "گاست و روش دوم از یک روش پیچیده‌تر به نام «خلاصه لیست» یا همان\n",
    "\n",
    "List Comprehension\n",
    "\n",
    "استفاده می‌کند. البته نیازی نیست جزئیات این دو روش را بدانید. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f6e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [['سلام', 'دنیا', '!'], ['این', 'یک', 'متن', 'فارسی', 'است', '.'], ['امیدوارم', 'که', 'این', 'درس', 'مفید', 'باشد', '.']]\n"
     ]
    }
   ],
   "source": [
    "# First way: Tokenize the words in each sentence\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "  tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "print('Tokenized:', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac623b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [['سلام', 'دنیا', '!'], ['این', 'یک', 'متن', 'فارسی', 'است', '.'], ['امیدوارم', 'که', 'این', 'درس', 'مفید', 'باشد', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Second way: Tokenizing each sentence using list comprehension\n",
    "print('Tokenized:', [tokenizer.tokenize(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a194a",
   "metadata": {},
   "source": [
    "### استفاده از نمونه مثالی\n",
    "\n",
    "حال از میان چندین جمله، بلندترین جمله را انتخاب می‌کنیم. سپس کلمه‌ای را که بیش از همه در این جملات متداول است، شناسایی می‌کنیم. ما از آیتم «حسابگر» یا همان \n",
    "\n",
    "`Counter`\n",
    "\n",
    " در کتابخانه\n",
    " \n",
    " `collections`\n",
    " \n",
    " برای پیدا کردن کلمات متداول استفاده خواهیم کرد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2258b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. '.' appears 10 times.\n",
      "2. 'گربه' appears 7 times.\n",
      "3. 'است' appears 6 times.\n",
      "Longest sentence: 'موش در حال حاضر در حیاط پنهان شده است.'\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from hazm import WordTokenizer, SentenceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"\"\"\n",
    "گربه در خانه است. او به دنبال موش است. موش در حال حاضر در حیاط پنهان شده است.\n",
    "گربه دوباره به دنبال موش است. موش سعی می کند از گربه فرار کند. گربه در خانه است.\n",
    "گربه بازی می کند. موش بازی می کند. گربه در خانه است. گربه به دنبال موش است.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the tokenizers\n",
    "word_tokenizer = WordTokenizer()\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "# Tokenize the text into words and sentences\n",
    "words = word_tokenizer.tokenize(text)\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counter = Counter(words)\n",
    "\n",
    "# Find the top three most common words\n",
    "top_three_common_words = word_counter.most_common(3)\n",
    "\n",
    "# Find the longest sentence\n",
    "longest_sentence = max(sentences, key=len)\n",
    "\n",
    "# Print the results\n",
    "for i, (word, count) in enumerate(top_three_common_words, start=1):\n",
    "    print(f\"{i}. '{word}' appears {count} times.\")\n",
    "print(f\"Longest sentence: '{longest_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5bce6",
   "metadata": {},
   "source": [
    "توجه داشته باشید که نقطه، `.`، اینجا به عنوان یک توکن مجزا به حساب آمده است، بنابراین اگر نمی‌خواهیم نقطه را در این بررسی قرار دهیم، باید آن را از مجموعه توکن‌ها حذف کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd467b2",
   "metadata": {},
   "source": [
    "### برچسب‌گذاری جزء کلام (پی‌او‌اِس)\n",
    "\n",
    "پی‌او‌اِس به روندی گفته می‌شود که طی آن اجزای را بر اساس دستورزبان دسته بندی و یا جایگاه هر کلمه در جمله را تعیین می‌کنیم. این روش به ما این امکان را می‌دهد تا کلماتی نیز «و» یا کلمات مشابه را از مجموعه کلمات مد نظرمان حذف کنیم.\n",
    "\n",
    "برای انجام این کار، کتابخانه `هضم` یک مدل برچسب‌زنی را ایجاد کرده است که می‌توان آن را از [صفحه گیت‌هاب هضم](https://github.com/roshan-research/hazm) این نوت‌بوک اما، این قابلیت از پیش بارگذاری شده و در پوشه منابع یا همان \n",
    "\n",
    "`resources/`\n",
    "\n",
    "این ریپازیتوری گرفته است. در حال حاضر ما دو دایرکتوری از ریپازیتوری مرجع فاصله داریم، بنابراین باید ../../ را در ابتدای فرمان مسیرمان بنویسیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4449597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger\n",
    "\n",
    "# Initialize the tagger\n",
    "tagger = POSTagger(model='../../resources/postagger.model')\n",
    "\n",
    "# Sample Persian sentence\n",
    "sentence = \"گربه در حال بازی کردن با موش است\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokenizer = WordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = tagger.tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f24959",
   "metadata": {},
   "source": [
    "می‌بینید که نقش هر کلمه در جمله مشخص شده است. می‌بینیم اسامی با حرف\n",
    "\n",
    "`N`\n",
    "\n",
    "و افعال با حرف\n",
    "\n",
    "`V`\n",
    "\n",
    "مشخص شده‌اند.\n",
    "\n",
    "اگر می‌خواهیم تمام اسامی را پیدا کنیم، باید یکی از فرامین زیر را به اجرا بگذاریم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04faf75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3410177439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_nouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_verbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpos_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mall_nouns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_tags' is not defined"
     ]
    }
   ],
   "source": [
    "all_nouns = []\n",
    "all_verbs = []\n",
    "for pos_tag in pos_tags:\n",
    "    if pos_tag[1] == 'N':\n",
    "        all_nouns.append(pos_tag[0])\n",
    "    elif pos_tag[1] == 'V':\n",
    "        all_verbs.append(pos_tag[0])\n",
    "print('Nouns:', all_nouns)\n",
    "print('Verbs:', all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4781f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3358017163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# یا یک روش پیشرفته‌تر که همان جواب را به ما می‌دهد.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nouns:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_tags' is not defined"
     ]
    }
   ],
   "source": [
    "# یا یک روش پیشرفته‌تر که همان جواب را به ما می‌دهد.\n",
    "print('Nouns:', [pos_tag[0] for pos_tag in pos_tags if pos_tag[1] == 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456af5bf",
   "metadata": {},
   "source": [
    "### قطعه‌بندی\n",
    "\n",
    "روش بالا برای پیدا کردن کلمات مجزا کارآیی دارد، ولی اگر بخواهیم عبارات یک جمله را پیدا کنیم، باید از روش دیگری استفاده کنیم. قطعه‌بندی یا همان\n",
    "\n",
    "`Chunking`\n",
    "\n",
    "به ما این امکان را می‌دهد تا عبارات یک جمله را بیابیم. `هضم` گزینه قطعه‌بندی را در اختیار ما قرار می‌دهد. حال می‌خواهیم از این قابلیت استفاده کنیم.\n",
    "\n",
    "دوباره، برای انجام این کار باید امکان قطعه‌بندی را از ریپازیتوری گیت‌هاب بارگذاری کنیم که اینجا ما این کار را انجام داده‌ایم و آن را در پوشه \n",
    "\n",
    "`resources/`\n",
    "\n",
    "گذاشته‌ایم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3acd629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3444668791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../resources/chunker.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'گربه در حال بازی کردن با موش است'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtree2brackets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chunker' is not defined"
     ]
    }
   ],
   "source": [
    "chunker = Chunker(model='../../resources/chunker.model')\n",
    "tagged = tagger.tag(word_tokenize('گربه در حال بازی کردن با موش است'))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72896f2",
   "metadata": {},
   "source": [
    "\n",
    "در اینجا\n",
    "\n",
    "\"VP\"\n",
    "\n",
    "برای عبارات جمله‌محور و\n",
    "\n",
    "\"NP\"\n",
    "\n",
    "برای عبارات اسم‌محور و\n",
    "\n",
    "\"PP\"\n",
    "\n",
    "برای عباراتی با محوریت مفعول باواسطه استفاده می‌شود.\n",
    "\n",
    "حال یک جمله ساده را به همین روش قطعه‌بندی می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5e2bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1192960455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimple_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"سگ زرد را دیدیم\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtree2brackets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "simple_sentence = \"سگ زرد را دیدیم\"\n",
    "tagged = tagger.tag(word_tokenize(simple_sentence))\n",
    "tree2brackets(chunker.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9792d8e",
   "metadata": {},
   "source": [
    "در اینجا\n",
    "\n",
    "`tree2brackets`\n",
    "\n",
    "روش ساده‌ا است برای شکستن جمله به عبارات مجزا. در عین انجام این کار ما همچنان می‌توانیم داده اصلی را که این عبارات از داخل آن استخراج شده است، ببینیم. این روش دارای ساختاری به نام درخت یا همان\n",
    "\n",
    "\"tree\"\n",
    "\n",
    "است که برخی کلمات به کلمات دیگری مرتبط است و برخی از آن‌ها نیز با هم دسته‌بندی شده‌اند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e4d1e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/2658507264.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunker' is not defined"
     ]
    }
   ],
   "source": [
    "chunked = chunker.parse(tagged)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904f5ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1826100274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# نمایش عبارت اول\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# نمایش نخستین کلمه عبارت اول\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunked' is not defined"
     ]
    }
   ],
   "source": [
    "# نمایش عبارت اول\n",
    "print(chunked[0])\n",
    "# نمایش نخستین کلمه عبارت اول\n",
    "print(chunked[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaad39",
   "metadata": {},
   "source": [
    "اگر بخواهیم عبارات جمله‌محور را استخراج کنیم، باید از لوپ\n",
    "\n",
    "`for`\n",
    "\n",
    "استفاده کنیم تا بتوانیم عبارات دسته‌بندی شده به عنوان\n",
    "\n",
    "`VP`\n",
    "\n",
    "را پیدا کنیم. این کار را می‌توانیم از طریق بررسی\n",
    "\n",
    "`.label()` \n",
    "\n",
    "هر قطعه انجام دهیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ec30e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/2978975910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mverb_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'VP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverb_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunked' is not defined"
     ]
    }
   ],
   "source": [
    "verb_phrase = None\n",
    "for chunk in chunked:\n",
    "    if chunk.label() == 'VP':\n",
    "        verb_phrase = chunk\n",
    "print(verb_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90b3b6",
   "metadata": {},
   "source": [
    "درک ساختار زبان، عامل کلیدی یک ان‌ال‌پی موفق است. تا اینجا ما روش‌های مختلفی را به کار بردیم تا بتوانیم به صورت خودکار بخش‌هایی از یک جمله و یا عبارات حاضر در یک جمله را پیدا کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74c885",
   "metadata": {},
   "source": [
    "### تجزیه بر اساس وابستگی\n",
    "\n",
    "تا این‌جا برخی از روش‌های اساسی تحلیل از طریق ان‌ال‌پی را آموختیم، ولی این روش‌ها معمولا روی شناسایی بخش مشخصی از متن و یا دسته‌بندی کردن کلمات تمرکز دارد. حال می‌آموزیم که کلمات چطور به هم دیگر مرتبط هستند. به این کار تجزیه بر اساس وابستگی یا همان\n",
    "\n",
    "dependency parsing\n",
    "\n",
    "می‌گوییم. تجزیه بر اساس وابستگی به ما این امکان را می‌دهد تا به عنوان مثال بفهمیم کدام بخش جمله، مفعول مستقیم مرتبط با فعل است و یا کدام اسم در جمله نقش مفعول را دارد. \n",
    "\n",
    "اما پیش از صحبت در مورد تجزیه بر اساس وابستگی، باید به یک موضوع دیگر نیز نگاهی اجمالی بیاندازیم.\n",
    "\n",
    "####  بُن‌واژه‌سازی\n",
    "بن‌واژه‌سازی یا همان\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "به روندی گفته می‌شود که طی آن اشکال مختلف یک کلمه را دسته‌بندی می‌کنیم. مثلا، «فروختن»، «می‌فروشد» با هم در یک گروه قرار می‌گیرند و «سگ» و «سگ‌ها» در یک گروه دیگر. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4289cbe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/2878042334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'سگ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'سگها'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize('سگ'))\n",
    "print(lemmatizer.lemmatize('سگها'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069cbfd",
   "metadata": {},
   "source": [
    "یکی از موارد استفاده این تکنیک این است که مثلا ما ممکن است در یک متن دنبال کلمه‌ای باشیم که ممکن است به اشکال مختلف نوشته شده باشد. بر این اساس می‌توانیم تکنیک بن‌واژه‌سازی را برای هر کلمه به کار ببریم و ببینیم آیا این بن‌واژه‌ها در متن ما موجود است یا خیر.\n",
    "\n",
    "توجه داشته باشید که برای استفاده از روش تجزیه بر اساس وابستگی، باید `جاوا` را نصب کنید. حتی اگر زمانی که برنامه را اجرایی می‌کنید، پیام خطا دریافت می‌کنید، مهم نیست. سعی کنید بفهمید که آن کد چه کاری قرار است انجام دهد. در این صورت، برای تجزیه بر اساس وابستگی  [«اوپن جِی‌دی‌کِِی»](https://openjdk.org/) را نصب کنید. بسته به نوع سیستم عاملتان، نحوه نصب «اوپن جی‌دی‌کی» متفاوت خواهد بود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d78726",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DependencyParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3315095002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimple_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'سگ زرد را دیدیم'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../resources/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdependency_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DependencyParser' is not defined"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'سگ زرد را دیدیم'\n",
    "parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer, working_dir='../../resources/')\n",
    "dependency_graph = parser.parse(word_tokenize(simple_sentence))\n",
    "print(dependency_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2c014",
   "metadata": {},
   "source": [
    "درک و دریافت خروجی اصلی این داده خام کار راحتی نیست، بنابراین از روش دیگری برای بررسی آن استفاده می‌کنیم.\n",
    "\n",
    "نخستین چیزی که باید به آن توجه کنید فرمت\n",
    "\n",
    "CoNLL\n",
    "\n",
    "است. می‌بینیم که سطر «روت» یا همان\n",
    "\n",
    "`ROOT`\n",
    "\n",
    "در پایین، نقطه آغاز گراف وابستگی را که با فعل آغاز می‌شود، به ما نشان می‌دهد. سپس سطر «او‌بی‌جی» یا همان\n",
    "\n",
    "`OBJ`\n",
    "\n",
    "را می‌بینیم که مفعول وابسته به فعل است. این روش برای درک نقش هر کلمه در جمله مناسب است، ولی مشخص نمی‌کند که این کلمات چطور به هم متصل هستند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0ba5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/462700547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "print(dependency_graph.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba424ef4",
   "metadata": {},
   "source": [
    "اگر با گراف داده آشنا باشید، می‌توانید از روش \n",
    "\n",
    "`to-dot`\n",
    "\n",
    "استفاده کنید که روشی است برای شکستن جملات. در پایین می‌بینیم که برای هر کلمه عددی تعیین شده است و هر کدام از این اعداد به اعداد دیگر متصل می‌شوند. می‌بینیم که جمله با رقم `۰` شروع شده که در سطر دوم به رقم `۴` متصل می‌شود که همان \n",
    "\n",
    "`ROOT`\n",
    "\n",
    "است. منظور از سطر دوم این سطر است:\n",
    "\n",
    "`0 -> 4 [label=\"ROOT\"]`\n",
    "\n",
    "در ادامه، می‌بینیم که رقم `۴` به رقم `۳` متصل می‌شود و رقم `۳` همان مفعول است و این تقسیم‌بندی به همین منوال ادامه پیدا می‌کند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee303d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/397579102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "print(dependency_graph.to_dot())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5217659",
   "metadata": {},
   "source": [
    "در پایان می‌توانیم با استفاده از پرانتز، جمله را بشکنیم تا بفهمیم کدام کلمات مرکز جمله را تشکیل می‌دهند و چگونه به باقی جمله مرتبط هستند. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22621fac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1696531074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "dependency_graph.tree().pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141ca5",
   "metadata": {},
   "source": [
    "### استفاده از گراف وابستگی\n",
    "\n",
    "حال که یک گراف داده تولید کرده‌ایم، یکی از آن‌ها را مورد مطالعه قرار می‌دهیم تا نشان دهیم چطور می‌توان به ریشه و مفعول جمله دست یافت. \n",
    "\n",
    "نکته کلیدی این است که گراف وابستگی از «بَند»، «نود» یا همان\n",
    "\n",
    "`node`\n",
    "\n",
    "تشکیل شده است. هر «نود» به یک ریشه یا «نود مرجع» وصل است و همچنین دارای «نودهای زیرمجموعه» است. در این‌جا، «نود» ما در اصلا یک کلمه در متن جمله است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57dbd06a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/4211694885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We will first visualize the root node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# We will first visualize the root node\n",
    "dependency_graph.root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b9ea0",
   "metadata": {},
   "source": [
    "در اینجا می‌بینیم که می‌توانیم کلمه مورد نظر را در کلید دیکشنری \n",
    "\n",
    "‍`word`\n",
    "\n",
    "پیدا کنیم و نوع کلمه در جمله را در کلید دیکشنری\n",
    "\n",
    "`rel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd58f61a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3397540291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroot_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroot_relationship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relationship: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_relationship\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "root_word = dependency_graph.root['word']\n",
    "root_relationship = dependency_graph.root['rel']\n",
    "print('Word: ', root_word)\n",
    "print('Relationship: ', root_relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34658520",
   "metadata": {},
   "source": [
    "حال مفعول یا همان\n",
    "\n",
    "`OBJ`\n",
    "\n",
    "جمله را پیدا می‌کنیم و بسته به این‌که مفعول به چند کلمه می‌تواند مستقیما متصل باشد، تمامی کلماتی را که مستقیما به مفعول مرتبط هستند، شناسایی می‌کنیم. این کار منجر به تولید یک گراف جستجوی ساده می‌شود و کاری را می‌کنم که به آن «جستجوی عرضی اول-سطح» یا همان\n",
    "\n",
    "breadth first search\n",
    "\n",
    "می‌گویند. هدف جستجوی برای پیدا کردن تمامی «نود»های حاضر در گراف است. به همین منظور، از از «نود» ریشه شروع می‌کنیم و مابقی «نود»های زیر «نود» ریشه را به آناضافه می‌کنیم و این کار را تا اضافه کردن و جستجوی تمامی «نود»ها ادامه می‌دهیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc26254",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3515863416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdependency_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_graph' is not defined"
     ]
    }
   ],
   "source": [
    "dependency_graph.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b03dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3465873912.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3465873912.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    get and remove the first item from the nodes_to_search list\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# لیستی می‌سازیم و تمامی «نود»ها را به آن اضافه می‌کنیم. از «نود» ریشه شروع می‌کنیم.\n",
    "\n",
    "nodes_to_search = [dependency_graph.root]\n",
    "\n",
    "# مفعول جمله را اینجا ذخیره می‌کنیم.\n",
    "\n",
    "object_nodes = []\n",
    "\n",
    "while len(nodes_to_search) > 0:\n",
    "    # نخستین مورد حاضر در لیست «نودز_تو_سِرچ» را پیدا و آن را حذف می‌کنیم \n",
    "    get and remove the first item from the nodes_to_search list\n",
    "    current_node = nodes_to_search.pop()\n",
    "    is_node_object = False\n",
    "    if current_node['rel'] == 'OBJ':\n",
    "        object_nodes.append(current_node)\n",
    "        # حال تمامی کلماتی را که مستقیما به این «نود» متصل هستند را اضافه می‌کنیم.\n",
    "        is_node_object = True\n",
    "\n",
    "    \n",
    "    # حال این «نود»های وابستگی را به «نودز_تو_سِرچ» اضافه می‌کنیم.\n",
    "    for dependency in current_node['deps'].values():\n",
    "        for node_index in dependency:\n",
    "            nodes_to_search.append(dependency_graph.nodes[node_index])\n",
    "            # اگر «نود» فعلی، نقش مفعول دارد، «نود»های وابسته را به «آبجکت_نودز» هم اضافه می‌کنیم.\n",
    "            if is_node_object:\n",
    "                object_nodes.append(dependency_graph.nodes[node_index])\n",
    "\n",
    "print('Objects:')\n",
    "for obj in object_nodes:\n",
    "    print(obj['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2099811",
   "metadata": {},
   "source": [
    "حال یک جمله پیچیده‌تر را تحلیل می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc4e6ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1560559932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcomplex_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.پسرم را به مدرسه بردم'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcomplex_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "complex_sentence = '.پسرم را به مدرسه بردم'\n",
    "complex_graph = parser.parse(word_tokenize(complex_sentence))\n",
    "print(complex_graph.to_dot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a96aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complex_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/2400875833.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplex_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'complex_graph' is not defined"
     ]
    }
   ],
   "source": [
    "complex_graph.tree().pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25311e4b",
   "metadata": {},
   "source": [
    "از همان کد بالا استفاده می‌کنیم، ولی این‌بار، تابعی به آن اضافه می‌کنیم تا بتوانیم تمامی مفعول‌های جمله را استخراج کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "834b3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تابعی تعریف می‌کنیم که بر اساس آن می‌توانیم به «نود» مفعول تمامی مفعول‌های مستقیم دست پیدا کنیم.\n",
    "def compute_direct_objects(sentence):\n",
    "    dependency_graph = parser.parse(word_tokenize(sentence))\n",
    "    \n",
    "    # حال لیستی تهیه می‌کنیم و تمامی «نود»ها را در آن جمع‌آوری می‌کنیم. از «نود» ریشه آغاز می‌کنیم.\n",
    "    nodes_to_search = [dependency_graph.root]\n",
    "    # مفعول جمله را اینجا ذخیره می‌کنیم.\n",
    "    object_nodes = []\n",
    "\n",
    "    while len(nodes_to_search) > 0:\n",
    "        # .نخستین مورد حاضر در لیست «نودز_تو_سِرچ» را پیدا و آن را حذف می‌کنیم \n",
    "\n",
    "        current_node = nodes_to_search.pop()\n",
    "        is_node_object = False\n",
    "        if current_node['rel'] == 'OBJ':\n",
    "            object_nodes.append(current_node)\n",
    "            # حال تمامی کلماتی را که مستقیما به این «نود» متصل هستند را اضافه می‌کنیم.\n",
    "            is_node_object = True\n",
    "\n",
    "\n",
    "        # حال این «نود»های وابستگی را به «نودز_تو_سِرچ» اضافه می‌کنیم.\n",
    "        for dependency in current_node['deps'].values():\n",
    "            for node_index in dependency:\n",
    "                nodes_to_search.append(dependency_graph.nodes[node_index])\n",
    "                # اگر «نود» فعلی، نقش مفعول دارد، «نود»های وابسته را به «آبجکت_نودز» هم اضافه می‌کنیم.\n",
    "                if is_node_object:\n",
    "                    object_nodes.append(dependency_graph.nodes[node_index])    \n",
    "    return object_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54abde07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/3172131877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_direct_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Objects:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/2600556581.py\u001b[0m in \u001b[0;36mcompute_direct_objects\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# تابعی تعریف می‌کنیم که بر اساس آن می‌توانیم به «نود» مفعول تمامی مفعول‌های مستقیم دست پیدا کنیم.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_direct_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdependency_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# حال لیستی تهیه می‌کنیم و تمامی «نود»ها را در آن جمع‌آوری می‌کنیم. از «نود» ریشه آغاز می‌کنیم.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "objects = compute_direct_objects(complex_sentence)\n",
    "print('Objects:')\n",
    "for obj in objects:\n",
    "    print(obj['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4a045",
   "metadata": {},
   "source": [
    "تجزیه بر اساس وابستگی یک مفهوم کلیدی در اِن‌اِل‌پی است و جزو نخستین مفاهیمی است که تدریس می‌شود تا نشان دهد چطور کامپیوترها می‌توانند نه تنها کلمات را درک کنن، بلکه بفهمند کلمات چطور به هم متصلند. به عنوان مثال،  جمله‌های «پسرم را به معلم نشان دادم» و «معلم را به پسرم نشان دادم» با هم تفاوت معنایی دارند و تجزیه بر اساس وابستگی آن‌ها به کامپیوتر کمک می‌کند تا این تفاوت را درک کند. \n",
    "\n",
    "#### موارد استفاده\n",
    "\n",
    "تجزیه بر اساس وابستگی از تمامی مفاهیمی که تا کنون آموخته‌ایم استفاده می‌کند تا جمله را تحلیل کند؛ از توکنیزه کردن جمله برای تجزیه کلمات گرفته، تا برچسب‌گذاری جزء کلام برای شناخت نقش کلمات در جمله و قطعه قطعه کردن جمله برای پیدا کردن عبارات. پس از تمامی این کارها، تجزیه بر اساس وابستگی شروع می‌کند به نحوه ارتباط کلمات یک جمله با یکدیگر.\n",
    "\n",
    "حال یک متن دو جمله‌ای را بررسی می‌کنیم. ابتدا متن را از طریق توکنیزه کردن در حد جملات می‌شکنیم و سپس مفعول‌های مستقیم جملات را از طریق کدی که در بالا نوشتیم شناسایی می‌کنیم. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c5551eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1436085993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ابزار توکنیزه جملات «هضم» را آماده می‌کنیم.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msentence_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# ابتدا یک متن حاوی دو جمله می‌سازیم.\n",
    "example_document = '''من یک کتاب خوب می‌خوانم. من یک ماشین جدید خریدم.'''\n",
    "\n",
    "# ابزار توکنیزه جملات «هضم» را آماده می‌کنیم.\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(example_document)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4250f23e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/_0qsl9196ld7tdz_2c4gc74c0000gn/T/ipykernel_22337/1036611989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# حال از تابع «کامپیوت_دیرِکت_آبجکتس» استفاده می‌کنیم تا مفعول‌های هر جمله را پیدا کنیم.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_direct_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# حال از تابع «کامپیوت_دیرِکت_آبجکتس» استفاده می‌کنیم تا مفعول‌های هر جمله را پیدا کنیم. \n",
    "for i in range(len(sentences)):\n",
    "    objects = compute_direct_objects(sentences[i])\n",
    "    print(sentences[i])\n",
    "    print([obj['word'] for obj in objects])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddea567",
   "metadata": {},
   "source": [
    "### استفاده از اِن‌اِل‌تی‌کِی(ابزار زبان طبیعی)»\n",
    "\n",
    "حال که از «هضم» برای تحلیل اِن‌اِل‌پی متن فارسی استفاده کردیم، زمان آن است تا همین کار را از طریق ن‌اِل‌تی‌کِی انجام دهیم، چرا که شاید لازم باشد متنی را که به زبان دیگری نوشته شده، تحلیل کنیم. اسامی استفاده شده در تابع ن‌اِل‌تی‌کِی مشابه «هضم» است. به همین خاطر به سرعت آن‌ها را بررسی می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d10a8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cf515",
   "metadata": {},
   "source": [
    "حال باید مدل‌های را بارگذاری کنیم و از طریق ان‌ال‌تی‌کی، برچسب‌گذاری جزء کلام (پی‌او‌اس) را انجام دهیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "296cf4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/denise/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/denise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d5d4a",
   "metadata": {},
   "source": [
    "حال برچسب‌گذاری جزء کلام را اجرایی می‌کنیم و نشان می‌دهیم که قطعه‌بندی در جمله انگلیسی به چه شکل است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fccf976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'VBG'), ('with', 'IN'), ('NLTK', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence in English\n",
    "sentence = \"I am learning Natural Language Processing with NLTK\"\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(word_tokenize(sentence))\n",
    "print('POS tags:', pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96a75356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: (S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  learning/VBG\n",
      "  Natural/NNP\n",
      "  Language/NNP\n",
      "  Processing/VBG\n",
      "  with/IN\n",
      "  NLTK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# قطعه‌بندی\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "print('Chunks:', tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a220d1",
   "metadata": {},
   "source": [
    "همان‌طور که می‌بینیم، بخش‌هایی از برچسب‌گذاری جزء کلام با نحوه برچسب‌گذاری `«هضم»` تفاوت دارد. دلیلش این است که `ان‌ال‌تی‌کی` از کنوانسیون «برچسب‌گذاری جزء کلام پِن» یا همان \n",
    "\n",
    "Penn Part of Speech Tags convention\n",
    "\n",
    "پیروی می‌کند. در این‌جا، کلمه «من» یا همان\n",
    "\n",
    "\"I\"\n",
    "\n",
    "یک «پی‌آر‌پی« یا همان\n",
    "\n",
    "`PRP`\n",
    "\n",
    "به حساب می‌آید که به معنای\n",
    "\n",
    "\"personal pronoun\"\n",
    "\n",
    "یا «ضمیر شخصی» است. فهرست کامل اجزای کلام  «برچسب‌گذاری جزء کلام پن» را می‌توانید در [این وبسایت](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html) متعلق به «دانشگاه نیویورک» پیدا کنید .\n",
    "\n",
    "برای پیدا کردن تمامی اسامی، باید تمامی اجزای جمله را که از نوع اسم هستند بیابید و سپس ببینید آیا این اسامی با اسامی مورد نظر شما مطابقت دارد یا نه."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cdfd9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['I', 'Natural', 'Language', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "# فهرستی از تمامی برچسب‌های کلامی مد نظر خود تهیه می‌کنیم.\n",
    "all_noun_types = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n",
    "\n",
    "# و سپس آن‌ها را با برچسب‌های موجود در جمله مطابقت می‌دهیم.\n",
    "nouns = []\n",
    "for tag in pos_tags:\n",
    "    if tag[1] in all_noun_types:\n",
    "        nouns.append(tag[0])\n",
    "\n",
    "print('Nouns:', nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab24b2",
   "metadata": {},
   "source": [
    "### تشخیص موجودیت‌های نامدار \n",
    "\n",
    "در بخش پایانی این مقدمه نشان می‌آموزیم که چطور می‌توان از طریق کتابخانه «اسپیسی»، دست به «تشخیص موجودیت‌های نامدار (اِن‌ای‌آر) زد. ان‌ای‌آر ضمایر شخصی را در جمله شناسایی می‌کند. از این طریق به عنوان مثال می‌توان فهمید در یک متن به چه افراد، شرکت‌ها و موجودیت‌ها اشاره شده است. \n",
    "\n",
    "ان‌ای‌آر یک روش آماری است؛ به این معنا که وابسته به نوعی آموزش و روش تحلیل داده است. بر این اساس، بسته به کیفیت مدل مورد استفاده، گاه ممکن است این روش تمامی موجودیت‌ها را نتواند بیابد. اسپیسی یک مدل یا قابلیت مشخصا طراحی شده برای زبان فارسی ندارد، اما به نوعی چند زبانه به حساب می‌آید و می‌تواند برخی تحلیل‌ها را به زبان فارسی انجام دهد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3230bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3d5f5",
   "metadata": {},
   "source": [
    "برای استفاده از اسپیسی، یا باید خودمان آن را تعلیم دهیم که این نیازمند داشتن یک کامپیوتر قوی و مقدار زیادی داده است، و یا از یک مدل تعلیم داده شده استفاده کنیم. برای این نوت‌بوک، از یک مدل از پیش تعلیم‌داده‌شده استفاده می‌کنیم. برای این کار، ابتدا باید این مدل را از طریق زیر بارگذاری کنیم. این مدل بر اساس انواعی از زبان‌ها تعلیم داده شده است و می‌تواند تا حدی زبان فارسی را تحلیل کند، ولی به طور کامل قابل اتکا نیست."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1e09006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xx-ent-wiki-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.5.0/xx_ent_wiki_sm-3.5.0-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from xx-ent-wiki-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (67.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/denise/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('xx_ent_wiki_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167a176",
   "metadata": {},
   "source": [
    "حال می‌توانیم این مدل تعلیم‌داده‌شده را در متغیر `ان‌ال‌پی` بریزیم و سپس هر جمله‌ای را فقط با کد\n",
    "\n",
    "`nlp(sentence)` \n",
    "\n",
    "بررسی کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1012a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "doc = nlp(\"محمد در حال شنا است\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ee91b",
   "metadata": {},
   "source": [
    "این کد بخش‌های زیادی از جمله را به ما نشان می‌دهد، از جمله نام یک شخص. برای پیدا کردن تمامی موجودیت‌های موجود در این جمله، تنها کاری که باید بکنیم این است که از کد\n",
    "\n",
    "`doc.ents.`\n",
    "\n",
    "استفاده کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a38e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(محمد در,)\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb511d",
   "metadata": {},
   "source": [
    "توجه داشته باشید که اینجا، کلمه در، به معنای داخل نیز به عنوان بخشی از  موجودیت «محمد» شناسایی شده است. این یک نمونه از محدودیت‌های ان‌ال‌تی‌کی در زبان فارسی است که به آن اشاره شد. \n",
    "\n",
    "برای شناسایی اسامی موجود در جمله، می‌توانیم از \n",
    "\n",
    "`.text`\n",
    "\n",
    "استفاده کنیم. اسپیسی اتریبیوت‌های دیگری نظیر\n",
    "\n",
    "`.label`\n",
    "\n",
    "هم دارد که چندان به کار زبان فارسی نمی‌آیند. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "478a423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('محمد در', 4317129024397789502)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e68f13",
   "metadata": {},
   "source": [
    "ان‌ای‌آر یک روش چالش‌برانگیز است و نیازمند آموزش بسیار از طریق داده. به همین خاطر، مدلی که در بالا به آن اشاره شد، همیشه به درستی کار نمی‌کند، چرا که همانطور که گفته شد، برای زبان فارسی طراحی نشده است. در درس ان‌ال‌پی آتی، فن‌آوری استفاده شده در این روش را بیشتر بررسی می‌کنیم و نشان می‌دهیم که با استفاده از کد\n",
    "\n",
    "`nlp(sentence)`\n",
    "\n",
    "در واقع چه کاری انجام می‌دهیم.\n",
    "\n",
    "### نتیجه‌گیری\n",
    "\n",
    "در این نوت‌بوک، تا حدی ایده‌های کلیدی ان‌ال‌پی را که نه تن‌ها در روش‌های ابتدایی، که در روش‌های پیشرفته ان‌ال‌پی نیز مورد استفاده قرار می‌گیرند، آموختیم. به عنوان مثال، توکنیزه کردن، روشی است که در مدل‌های پیشرفته نظیر جی‌پی‌تی-۴ هم مورد استفاده قرار می‌گیرد.  \n",
    "\n",
    "حال می‌دانیم چطور می‌توان اسامی موجود در یک جمله را شناسایی و تحلیل کرد. این کار را می‌توان در متن‌های طولانی نیز انجام داد. پس از این شناسایی، از طریق تجزیه بر اساس وابستگی می‌توانیم یک قدم فراتر برویم و مفعول‌های وابسته به هر فعل را پیدا کنیم.\n",
    "\n",
    "در پایان نیز، اصول ابتدایی استفاده از اسپیسی را بررسی کردیم و نشان دادیم چطور از طریق آن می‌توان موجودیت‌های اشاره شده در یک متن را شناسایی کرد. این مقدمه‌ای بود بر بخشی از آن‌چه در متدهای ان‌ال‌پی مورد استفاده قرار می‌گیرد."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
